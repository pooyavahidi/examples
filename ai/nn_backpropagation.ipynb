{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we go through the forward propagation and then step by step details of the backpropagation using builtin [computational graph]() in PyTorch.\n",
    "\n",
    "Our problem is a binary classification. We have 2 input features and our dataset has 2 samples. We define our neural network with the following architecture: \n",
    "- Layer 0 (input layer): 2 features\n",
    "- Layer 1: Fully connected layer with 3 neurons and ReLU activation function.\n",
    "- Layer 2: Fully connected layer with 2 neurons and ReLU activation function.\n",
    "- Layer 3 (output): Fully connected layer with 1 neuron (output) and Sigmoid activation function.\n",
    "\n",
    "In this example we use a batch dataset with 2 samples. The input $X$ and target $Y$ are defined as follows:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "Which means, for example 1 $x_1 = 1$ and $x_2 = 2$ and the target class $y = 0$.\n",
    "\n",
    "Recall that we maintain each sample in **rows** and features in **columns**. So, each row of $X$ and $Y$ is associated with one sampleDataset with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = torch.tensor([[0.0], [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Define the model architecture (Layers and nodes)\n",
    "        self.linear1 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.linear2 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.linear3 = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward Propagation happens here.\n",
    "        # It takes the input tensor x and returns the output tensor for each\n",
    "        # layer by applying the linear transformation first and then the\n",
    "        # activation function.\n",
    "        # It start from layer 1 and goes forward layer by layer to the output\n",
    "        # layer.\n",
    "\n",
    "        # Layer 1 linear transformation\n",
    "        Z1 = self.linear1(x)\n",
    "        # Layer 1 activation\n",
    "        A1 = F.relu(Z1)\n",
    "\n",
    "        # Layer 2 linear transformation\n",
    "        Z2 = self.linear2(A1)\n",
    "        # Layer 2 activation\n",
    "        A2 = F.relu(Z2)\n",
    "\n",
    "        # Layer 3 (output layer) linear transformation\n",
    "        Z3 = self.linear3(A2)\n",
    "        # Layer 3 activation\n",
    "        A3 = F.sigmoid(Z3)\n",
    "\n",
    "        # Print the intermediate results\n",
    "        print(\n",
    "            f\"Z1:\\n{Z1}\\nA1:\\n{A1}\\nZ2:\\n{Z2}\\nA2:\\n{A2}\\nZ3:\\n{Z3}\\nA3:\\n{A3}\"\n",
    "        )\n",
    "\n",
    "        # Output of the model (prediction)\n",
    "        return A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for classification problems, when we use the Sigmoid or Softmax activation function in the output layer, we defer the activation of the output layer to the outside the model. In other words, the output layer just do the linear transformation $Z$ and output the [logits](). Then we apply the activatio function (Sigmoid or Softmax) outside the model on the logits to get the predicted probabilities.\n",
    "\n",
    "This approach is the same for both inference and training.\n",
    "\n",
    "However, in this example, for simplicity and focus on the backpropagation, we will include the Sigmoid activation function in the output layer. So, in this example, the output layer will output the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (linear3): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the initial weights and biases of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[ 0.1059,  0.2517],\n",
      "        [-0.0551, -0.5693],\n",
      "        [ 0.3489,  0.0364]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 0.6874, -0.2069, -0.2842], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.1931, -0.0434,  0.5419],\n",
      "        [ 0.5230, -0.5720,  0.4680]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([ 0.5208, -0.4686], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[-0.0407, -0.1357]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([-0.0221], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    for i, child in enumerate(model.children()):\n",
    "        print(f\"Layer {i+1}: {type(child).__name__}\")\n",
    "        child_parameters = dict(child.named_parameters())\n",
    "\n",
    "        for name, param in child_parameters.items():\n",
    "            print(f\"\\n{name}: {param.size()} {param}\")\n",
    "            print(f\"{name}.grad:\\n{param.grad}\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, gradients of parameters are `None` since we haven't computed any gradients yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example for simplicity and having reproducible results, we'll set the weights and biases manually. Let's say we have the following weights and biases:\n",
    "\n",
    "Similar to the way that PyTorch creates weights matrices:\n",
    "- Each row of $W^{[l]}$ is associated with one neuron in the layer $l$. For example, in layer 1, We have 3 neurons, so we have 3 rows in $W^{[1]}$.\n",
    "- Each column of $W^{[l]}$ is associated with one feature of input values. For example, the number of columns in $W^{[1]}$ is equal to the number of features in the input layer $X$. We have 2 features in the input layer $X$, so we have 2 columns in $W^{[1]}$.\n",
    "\n",
    "\n",
    "**Layer 1 (3 neurons):**\n",
    "$$W^{[1]} = \\begin{bmatrix} -1 & 2 \\\\ 3 & 0.5 \\\\ -0.1 & -4\\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[1]} = \\begin{bmatrix} 1 & -2 & 0.3 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**Layer 2 (2 neurons):**\n",
    "$$W^{[2]} = \\begin{bmatrix} 0.5 & 1 & -2 \\\\ 0.7 & 0.1 & 0.3\\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[2]} = \\begin{bmatrix} -4 & 5 \\end{bmatrix}$$\n",
    "\n",
    "**Layer 3 (output):**\n",
    "$$W^{[3]} = \\begin{bmatrix} 0.5 & -0.3 \\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[3]} = \\begin{bmatrix} 0.1 \\end{bmatrix}$$ \n",
    "\n",
    "Note: The number of weight and biases are independent of the number of training samples (in any batch or entire dataset). The whole point of training with sample datasets is to optimize these parameters by exposing them to the entire dataset through cycle of forward and backward propagation. So, no matter what is the size of the dataset, the number of parameters in the model is fixed and defined by the architecture of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "W_1 = torch.tensor([[-1.0, 2.0], [3.0, 0.5], [-0.1, -4.0]], requires_grad=True)\n",
    "b_1 = torch.tensor([1.0, -2.0, 0.3], requires_grad=True)\n",
    "\n",
    "# Layer 2\n",
    "W_2 = torch.tensor([[0.5, 1.0, -2.0], [0.7, 0.1, 0.3]], requires_grad=True)\n",
    "b_2 = torch.tensor([-4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Layer 3 (Output layer)\n",
    "W_3 = torch.tensor([[0.5, -0.3]], requires_grad=True)\n",
    "b_3 = torch.tensor([0.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set these weights and biases in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.linear1.weight.data.copy_(W_1)\n",
    "model.linear1.bias.data.copy_(b_1)\n",
    "\n",
    "model.linear2.weight.data.copy_(W_2)\n",
    "model.linear2.bias.data.copy_(b_2)\n",
    "\n",
    "model.linear3.weight.data.copy_(W_3)\n",
    "model.linear3.bias.data.copy_(b_3)\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the forward propagation using the current weights and biases, and the input $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1:\n",
      "tensor([[  4.0000,   2.0000,  -7.8000],\n",
      "        [  6.0000,   9.0000, -16.0000]], grad_fn=<AddmmBackward0>)\n",
      "A1:\n",
      "tensor([[4., 2., 0.],\n",
      "        [6., 9., 0.]], grad_fn=<ReluBackward0>)\n",
      "Z2:\n",
      "tensor([[ 0.0000,  8.0000],\n",
      "        [ 8.0000, 10.1000]], grad_fn=<AddmmBackward0>)\n",
      "A2:\n",
      "tensor([[ 0.0000,  8.0000],\n",
      "        [ 8.0000, 10.1000]], grad_fn=<ReluBackward0>)\n",
      "Z3:\n",
      "tensor([[-2.3000],\n",
      "        [ 1.0700]], grad_fn=<AddmmBackward0>)\n",
      "A3:\n",
      "tensor([[0.0911],\n",
      "        [0.7446]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "output = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Forward Propagation]() we feed the input $X$ (which could be a single sample, a batch of samples, or the entire dataset) to the model and then compute the output of first layer, then give that output to the next layer (as input) and compute the output of the next layer, and so on until we reach the output layer.\n",
    "\n",
    "In each layer, we have two steps of computation:\n",
    "\n",
    "**1. Linear Transformation:**<br>\n",
    "$$Z^{[l]} = A^{[l-1]} \\cdot {W^{[l]}}^\\top + {\\vec{\\mathbf{b}}}^{[l]}$$\n",
    "\n",
    "**2. Activation Function:**<br>\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "By convention, we consider $X$ as the layer $0$. So, $A^{[0]} = X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the output of the layer $1$.\n",
    "\n",
    "**Layer 1:**\n",
    "$$Z^{[1]} = X \\cdot {W^{[1]}}^\\top + {\\vec{\\mathbf{b}}}^{[1]}$$\n",
    "\n",
    "$$Z^{[1]} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 & 3 & -0.1 \\\\ 2 & 0.5 & -4 \\end{bmatrix} + \\begin{bmatrix} 1 & -2 & 0.3 \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = \\begin{bmatrix} 3 & 4 & -8.1 \\\\ 5 & 11 & -16.3 \\end{bmatrix} + \\begin{bmatrix} 1 & -2 & 0.3 \\end{bmatrix}$$\n",
    "\n",
    "We broadcast the bias vector to the shape of $(2, 3)$ and add it to the dot product of $X$ and $W^{[1]}$.\n",
    "\n",
    "$$Z^{[1]} = \\begin{bmatrix} 3 & 4 & -8.1 \\\\ 5 & 11 & -16.3 \\end{bmatrix} + \\begin{bmatrix} 1 & -2 & 0.3 \\\\ 1 & -2 & 0.3 \\end{bmatrix} = \\begin{bmatrix} 4 & 2 & -7.8 \\\\ 6 & 9 & -16 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our calculation using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_np = X.numpy()\n",
    "W_1_np = W_1.detach().numpy()\n",
    "b_1_np = b_1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_1 (using numpy):\n",
      "[[  4.         2.        -7.8     ]\n",
      " [  6.         9.       -15.999999]]\n"
     ]
    }
   ],
   "source": [
    "Z_1_np = np.dot(X_np, W_1_np.T) + b_1_np\n",
    "\n",
    "print(f\"Z_1 (using numpy):\\n{Z_1_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: due to how floating point arithmetic works in computer hardware, the result of $-16.3 + 0.3$ is not exactly $-16$ but a number very close to $-16$ like $-15.9999...$.\n",
    "> For example, `print(0.1 + 0.2)` will output `0.30000000000000004` instead of `0.3`. This is not a bug, it's a limitation of floating point arithmetic in computers and it's not specific to Python or PyTorch.\n",
    "> \n",
    "> However, this is not a problem in practice in machine learning and deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the activation of layer 1 using the [ReLU]() activation function.\n",
    "\n",
    "$$A^{[1]} = \\text{ReLU}(Z^{[1]})$$\n",
    "\n",
    "$$A^{[1]} = \\begin{bmatrix} \\text{ReLU}(4) & \\text{ReLU}(2) & \\text{ReLU}(-7.8) \\\\ \\text{ReLU}(6) & \\text{ReLU}(9) & \\text{ReLU}(-16) \\end{bmatrix}$$\n",
    "\n",
    "We know that the ReLU function is defined as:\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "We apply ReLU element-wise to the matrix $Z^{[1]}$. So, the output of the layer 1 is: \n",
    "\n",
    "$$A^{[1]} = \\begin{bmatrix} 4 & 2 & 0 \\\\ 6 & 9 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our calculation using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_1 (using numpy):\n",
      "[[4. 2. 0.]\n",
      " [6. 9. 0.]]\n"
     ]
    }
   ],
   "source": [
    "A_1_np = relu(Z_1_np)\n",
    "\n",
    "print(f\"A_1 (using numpy):\\n{A_1_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we compare this result with the PyTorch output, we see that they are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the output of the layer 2.\n",
    "\n",
    "**Layer 2:**\n",
    "\n",
    "$$Z^{[2]} = A^{[1]} \\cdot {W^{[2]}}^\\top + {\\vec{\\mathbf{b}}}^{[2]}$$\n",
    "\n",
    "$$Z^{[2]} = \\begin{bmatrix} 4 & 2 & 0 \\\\ 6 & 9 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 & 0.7 \\\\ 1 & 0.1 \\\\ -2 & 0.3 \\end{bmatrix} + \\begin{bmatrix} -4 & 5 \\end{bmatrix}$$\n",
    "\n",
    "Which equals to:\n",
    "\n",
    "$$Z^{[2]} = \\begin{bmatrix} 0 & 8 \\\\ 8 & 10.1 \\end{bmatrix}$$\n",
    "\n",
    "And then applying the ReLU activation function:\n",
    "\n",
    "$$A^{[2]} = \\begin{bmatrix} 0 & 8 \\\\ 8 & 10.1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can keep going **forward** and compute the outputs (linear transformations and activations) layer by layer until we reach the output layer.\n",
    "\n",
    "The output of the output layer is the **prediction** of the model which in this case is the predicted probability of binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Loss and Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the cost will provide the error of our model in respect to the labels (target value $Y$). \n",
    "\n",
    "The [cost]() function is usually the average of the [loss]() function over all the samples in the batch (which pass through in the forward propagation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed [here](), the loss function for binary classification is the binary cross-entropy loss which is defined as:\n",
    "\n",
    "$$L(\\theta)^{(i)} = -y^{(i)} \\log(f_{\\theta}(x^{(i)})) - (1-y^{(i)}) \\log(1 - f_{\\theta}(x^{(i)}))$$\n",
    "\n",
    "Where:\n",
    "- $i$ is the index of the sample in the batch.\n",
    "- $y$ is the target value (label) of the $i$-th sample.\n",
    "- $\\theta$ is the model's parameters (weights and biases).\n",
    "- $f_{\\theta}$ is the model's function which produces the predicted probability based on the input $x$ and the model's parameters $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [cost]() function is the average of the loss function over all the samples in the batch.\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\theta)^{(i)}$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of samples in the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pytorch builtin cost function we can calculate the cost as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.19522885978221893\n"
     ]
    }
   ],
   "source": [
    "cost = F.binary_cross_entropy(output, Y)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, for more stable computation, in practice, we usually don't include the activation function in the output layer. So, the output of the model is the linear transformation $Z$ of the output layer (logits).  \n",
    "\n",
    "In this example, for simplicity, we include the Sigmoid activation function in the output layer. So, the `output` is the predicted probabilities. We will use `binary_cross_entropy()` loss function to calculate the loss. If we had deferred the activation function to the outside of the model, then the output would be the logits of the output layer, which then we should have used `binary_cross_entropy_with_logits()` loss function instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how this cost is calculated. We'll use numpy to calculate the cost manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss for example 1:**\n",
    "\n",
    "The input and target of the first sample are:\n",
    "$$x^{(1)} = \\begin{bmatrix} 1 & 2 \\end{bmatrix}, \\quad y^{(1)} = 0$$\n",
    "\n",
    "The predicted probability of the first sample is:\n",
    "$$f_{\\theta}(x^{(1)}) = {a^{[3]}}^{(1)} = 0.0911$$\n",
    "\n",
    "So, the loss of the first sample is:\n",
    "\n",
    "$$L(\\theta)^{(1)} = -0 \\times \\log(0.0911) - (1-0) \\times \\log(1 - 0.0911) = 0.0955$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_np = Y.numpy()\n",
    "A3_np = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "X: [1. 2.]\n",
      "Y: [0.]\n",
      "Output: [0.09112295]\n",
      "Loss: [0.09554543]\n"
     ]
    }
   ],
   "source": [
    "# Get the first row of the input X\n",
    "X_1 = X_np[0]\n",
    "\n",
    "# Get the first row of the output Y\n",
    "Y_1 = Y_np[0]\n",
    "\n",
    "# Get the output of the model for the first row\n",
    "A3_1 = A3_np[0]\n",
    "\n",
    "# Calculate the loss for the first example\n",
    "loss_1 = -Y_1 * np.log(A3_1) - (1 - Y_1) * np.log(1 - A3_1)\n",
    "\n",
    "print(f\"Example 1:\\nX: {X_1}\\nY: {Y_1}\\nOutput: {A3_1}\\nLoss: {loss_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss for example 2:**\n",
    "\n",
    "The input and target of the first sample are:\n",
    "$$x^{(2)} = \\begin{bmatrix} 3 & 4 \\end{bmatrix} \\quad y^{(2)} = 1$$\n",
    "\n",
    "The predicted probability of the first sample is:\n",
    "$$f_{\\theta}(x^{(2)}) = {a^{[3]}}^{(2)} = 0.0.7446$$\n",
    "\n",
    "So, the loss of the first sample is:\n",
    "\n",
    "$$L(\\theta)^{(2)} = -1 \\times \\log(0.7446) - (1-1) \\times \\log(1 - 0.7446) = 0.2949$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "X: [3. 4.]\n",
      "Y: [1.]\n",
      "Output: [0.7445969]\n",
      "loss: [0.29491228]\n"
     ]
    }
   ],
   "source": [
    "X_2 = X_np[1]\n",
    "Y_2 = Y_np[1]\n",
    "A3_2 = A3_np[1]\n",
    "\n",
    "loss_2 = -Y_2 * np.log(A3_2) - (1 - Y_2) * np.log(1 - A3_2)\n",
    "\n",
    "print(f\"Example 2:\\nX: {X_2}\\nY: {Y_2}\\nOutput: {A3_2}\\nloss: {loss_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the cost using numpy. As we discussed, the cost is the average of the loss over all the samples in the batch. In this case, we have only 2 samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (manual): [0.19522884]\n"
     ]
    }
   ],
   "source": [
    "cost_np = (loss_1 + loss_2) / 2\n",
    "print(f\"Cost (manual): {cost_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see we have reached the same value for the cost as PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we calculate the gradients of the loss function with respect to each parameter of the model. As we discussed in the [Backward Propagation](), we start from the last node of the computational graph (the cost node) and then calculate the partial derivative (gradient) of the loss with respect to each part of the graph step by step in backward direction until we reach to all the parameters of the model.\n",
    "\n",
    "**Using Chain Rule:**<br>\n",
    "We can see the whole model as a huge composite function which is made of many smaller functions (linear transformation and activation function of each layer). These functions are composed together layer by layer like a chain. So, in simple terms we can say that we use chain rule to calculate the gradient of the loss with respect to each parameter from the most outer function (cost) to the most inner function (parameters of the model).\n",
    "\n",
    "**Gradient of Loss vs Cost**:<br>\n",
    "We calculate the partial derivative of the **loss** with respect to each parameter of the model. That gives us the gradient of the loss with respect to each parameter for **one single sample**. Then we calculate the average of these gradients (mean gradient) over all the samples in the batch. In that case, we can say we have calculated the gradient of the **cost** with respect to each parameter of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the backward propagation by first defining our optimizer. We use [Adam]() variation of the [SGD (Stochastic Gradient Descent)]() algorithm to calculate the gradients and then update the parameters of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Adam optimizer with learning rate of 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we usually set all the gradients to zero before starting any new computation. Since PyTorch stores the gradients in the parameters, resetting them (using `zero_grad()`) is a good practice before calculating new gradients.\n",
    "\n",
    "In this particular example, as we haven't calculated any gradients yet, the gradients are `None`. So, resetting them has no effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each parameter (which has `requires_grad=True`) has a `grad` attribute which stores the gradient of the loss with respect to that parameter. We can see that all of our parameters currently has `None` as their gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the backpropagation by calling the `backward()` method on the last node of the computational graph (the cost node). This will start the backward step by step calculation of the gradients of the cost with respect to each parameter of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (compute the gradients)\n",
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-0.0249, -0.0396],\n",
      "        [-0.1814, -0.2428],\n",
      "        [ 0.0000,  0.0000]])\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0147, -0.0614,  0.0000])\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-0.3831, -0.5747,  0.0000],\n",
      "        [ 0.1752,  0.3175,  0.0000]])\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0639,  0.0246])\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-1.0216, -0.9253]])\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0821])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the backpropagation manually step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we start with the final node and walk backward the computational graph. So, we start with the cost node and calculate the gradient of the cost with respect to the output of the output layer.\n",
    "\n",
    "Also, recall the word **partial derivative** and **gradient** mean the same thing and we use them interchangeably.\n",
    "\n",
    "As we discussed earlier, we calculate the gradient of **loss** with respect to each parameter of the model. We do this for all of the calculations. At the end, we calculate the average of these gradients (mean gradient) over all the samples in the batch to give the gradient of the **cost** with respect to each parameter of the model.\n",
    "\n",
    "So, all the following steps are computing the gradient of the **loss** with respect to each parameter of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Partial Derivative of the Loss with respect to the Output of Layer 3\n",
    "\n",
    "**Loss for example 1:**\n",
    "\n",
    "$$L(\\theta)^{(1)} = -y^{(1)} \\log(f_{\\theta}(x^{(1)}) - (1-y^{(1)}) \\log(1 - f_{\\theta}(x^{(1)}))$$\n",
    "\n",
    "The output of the model $f_{\\theta}(x^{(1)})$ is the output of the output layer $A^{[3]}$. So:\n",
    "\n",
    "$$L(\\theta)^{(1)} = -y^{(1)} \\log({a^{[3]}}^{(1)}) - (1-y^{(1)}) \\log(1 - {a^{[3]}}^{(1)})$$\n",
    "\n",
    "Now let's calculate partial derivative of $L(\\theta)^{(1)}$ with respect to ${a^{[3]}}^{(1)}$.\n",
    "\n",
    "$$\\frac{\\partial L(\\theta)^{(1)}}{\\partial {a^{[3]}}^{(1)}} = -\\frac{y^{(1)}}{{a^{[3]}}^{(1)} } + \\frac{1-y^{(1)}}{1 - {a^{[3]}}^{(1)} }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: For calculating the derivative of $\\log(1 - x)$, we can also use the chain rule.\n",
    "> $$f(x) = \\log(1 - x)$$\n",
    "> $$u = 1 - x$$\n",
    "> $$f(u) = \\log(u)$$\n",
    "> $$\\frac{df(x)}{dx} = \\frac{df(u)}{du} \\cdot \\frac{du}{dx}$$\n",
    "> $$\\frac{df(x)}{dx} = \\frac{1}{u} \\cdot -1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
