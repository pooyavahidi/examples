{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use:\n",
    "- `scikit-learn` library to train a linear regression model\n",
    "- `seaborn` and `matplotlib` libraries to visualize the data and the model\n",
    "- California Housing dataset from `scikit-learn` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in [ML Workflow](../ml_workflow.md), there are key steps before we are ready for modelling. Those steps are data engineering, EPA (Exploratory Data Analysis), and feature engineering. In this simple example, we touch on those steps breifly.\n",
    "\n",
    "## Data Engineering\n",
    "\n",
    "In this example, our training dataset is reading to use from the `sklearn.datasets` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "In this example, the data engineering steps (collect, cleanse and preprocess the data) are already done for us. So, we simply use the California Housing dataset from `sklearn.datasets` module. This dataset has around 20,000 samples and 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset.\n",
    "housing_data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) and Feature Engineering\n",
    "\n",
    "We will briefly [explore and visualize](../ml_workflow.md#visualization-techniques)   the data and their relationship. Also, we will briefly touch on common steps of _Feature Selection_, _Feature Creation_ and _Feature Scaling_. \n",
    "\n",
    "Let's start getting familiar with our data.  The goal of this step is to first understand our data, and then to engineer the features in a way that our dataset is ready for training a high performing model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape and Types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the type and shapes of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the input features.\n",
    "X = housing_data.data\n",
    "\n",
    "# y is the target/label\n",
    "y = housing_data.target\n",
    "\n",
    "print(f\"X type: {type(X)}, X shape: {X.shape}\")\n",
    "print(f\"y type: {type(y)}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is a matrix of `m` samples and `n` features. Here `m = 20640` and `n = 8`. So, we have 8 features and total of 20640 samples.\n",
    "\n",
    "`y` is a vector of `m` samples. So, since here we are doing a _superivsed learning_ task, we should have target values for each of the samples. So, `y` is a vector of 20640 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X values:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\ny values in $100,000:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect each of the examples individually by using `pandas` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first row of X and its target y values:\n",
    "print(\"\\nFirst row of X and it's target value y:\")\n",
    "print(\"X_1:\")\n",
    "print(X.iloc[0])\n",
    "print(\"y_1:\")\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Outliers\n",
    "Now let's explore our data to check for any outliers. Detecting outliers is important as they can have a disproportionate effect on the model and lean to poor performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of each feature\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we already can see some outliers. For example, the `AveRooms` feature has some very high values e.g. 141 which is probably an outlier for a typical housing dataset. Or the number of occupants per household `AveOccup` has some very high values e.g. 1243. Those are probably outliers.\n",
    "\n",
    "However before we decide to remove them, let's visualize the data and see if we can get more insights. We'll use `seaborn` and `matplotlib` libraries for visualization. \n",
    "\n",
    "> Note: EDA and Feature Engineering is a process that needs to be done carefully and iteratively. This process needs a lot of domain knowledge and experience. For example, a valid but rare case might be considered as an outlier, etc.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box Plot**  \n",
    "A box plot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the box plot for one of the features `AveRooms`. Depending on the domain and the data, we can decide which features to start with for the clean up process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=X[\"AveRooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplot:\n",
    "- **Top Line (Upper Whisker)**: The top line represents the Q3, which the value below which 75% of the data falls.\n",
    "- **Middle Line**: The middle box also called **Interquartile Range (IQR)** which represents the median, which is the middle value of the dataset. $IQR = Q3 - Q1$\n",
    "- **Bottom Line (Lower Whisker)**: The bottom line represents the Q1, which the value below which 25% of the data falls. \n",
    "\n",
    "**Beyond the Whiskers**: The small circles are the indicator of potential outliers, as they fall beyond the acceptable range of typical data variability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the outliers\n",
    "We can start by determining the range of acceptable values using the **IQR (Interquartile Range)** method, which is standard for identifying outliers. However, we can apply a **softer approach** by increasing the threshold multiplier (e.g., $k = 2.5$ instead of the usual $1.5$) to only exclude extreme outliers. \n",
    "\n",
    "\n",
    "1. **Compute IQR:**\n",
    "   - Calculate $Q1$ (25th percentile) and $Q3$ (75th percentile) of `AveRooms`.\n",
    "   - Compute the IQR: $ \\text{IQR} = Q3 - Q1 $.\n",
    "   - Define thresholds:\n",
    "    $$ \\text{Lower Bound} = Q1 - k \\times \\text{IQR} $$\n",
    "    $$ \\text{Upper Bound} = Q3 + k \\times \\text{IQR} $$\n",
    "     We'll start with a **softer multiplier** like $k = 2.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Q1, Q3, and IQR\n",
    "Q1 = X[\"AveRooms\"].quantile(0.25)\n",
    "Q3 = X[\"AveRooms\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds (using a softer threshold with k = 2.5)\n",
    "k = 2.5\n",
    "lower_bound = Q1 - k * IQR\n",
    "upper_bound = Q3 + k * IQR\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "print(f\"Lower bound: {lower_bound}\")\n",
    "print(f\"Upper bound: {upper_bound}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Filter Out Outliers:**\n",
    "   - Remove rows where the `AveRooms` value is outside the bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter the dataset\n",
    "\n",
    "# Let's make a copy of the original dataset\n",
    "original_X = X.copy()\n",
    "original_y = y.copy()\n",
    "\n",
    "# Filter the dataset based on the bounds\n",
    "X = X[(X[\"AveRooms\"] >= lower_bound) & (X[\"AveRooms\"] <= upper_bound)]\n",
    "\n",
    "# Filter the target variable based on the filtered features\n",
    "y = y[X.index]\n",
    "\n",
    "print(f\"Original dataset size: {original_X.shape[0]}\")\n",
    "print(f\"Cleaned dataset size: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned up removed a tiny fraction of the data, which is a good sign that the data is not too noisy, and give us confidence that we probably removed the outliers, not the valid data.\n",
    "\n",
    "3. **Validate:**\n",
    "   - Visualize the boxplot again to ensure the outliers are handled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=X[\"AveRooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is a better plot now. We can see the obvious outliers are removed. You can continue this by adjusting the threshold of $k$ to lower or higher values and see how it affects the data.\n",
    "\n",
    "We can repeat the same process for other features as well and clean up the outliers. Although, we may decide to have a light clean up first, then go through the next steps which help us decide about the feature selection. Then after knowing the chosen features, we can come back to this step and do a more thorough clean up.\n",
    "\n",
    "> Note: EDA and Feature Engineering (similar to the whole ML process) is an **empirical process** and needs to be done **iteratively**. We may need to come back to this step after the feature selection and engineering steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    "This is a useful way to visualize the relationship between features (and features-target) which can help us in feature selection. This view in particular is useful to detect multicollinearity (when two or more features are highly correlated, i.e move of one, impact the other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation**    \n",
    " means that those two features are moving together. Correlation values range from -1 to 1. For example:\n",
    "- Positive correlation is when one feature increases, the other feature also increases.\n",
    "- Negative correlation is when one feature increases, the other feature decreases.\n",
    "\n",
    "As this influence from a feature to another increases, the correlation value moves towards 1 or -1. For example, two features with a correlation value of $0.7$ are much more correlated than two features with a correlation value of $0.2$. Similarly, two features with a correlation value of $-0.7$ are much more negatively correlated than two features with a correlation value of $-0.2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-collinearity Detection**  \n",
    "A common use of heatmap is to detect **multi-collinearity** between features. Multi-collinearity is a phenomenon in which two or more features in a dataset are highly correlated. This can cause problems in the model, such as unstable coefficients, and it can make it difficult to determine the effect of each feature on the target variable. In which those cases, we either:\n",
    "- Remove one of the features (Feature Selection).\n",
    "- Combine the features and create a new feature from them (New Feature Creation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(X, y):\n",
    "    data_combined = X.copy()\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    corr = data_combined.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following guide for interpreting correlation thresholds: \n",
    "\n",
    "- Negligible: $|r| < 0.1$ (often ignored)\n",
    "- Weak: $0.1 \\leq |r| < 0.3$\n",
    "- Moderate: $0.3 \\leq |r| < 0.5$\n",
    "- Strong: $0.5 \\leq |r| < 0.7$\n",
    "- Very Strong: $|r| \\geq 0.7$\n",
    "\n",
    "For feature selection, prioritize $|r| \\geq 0.3$, and focus on $|r| \\geq 0.5$ for strong relationships unless domain knowledge dictates otherwise.\n",
    "\n",
    "> The above is not a strict rule, but a guideline which can be adjusted based on the domain knowledge and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Analyze this heatmap: \n",
    "\n",
    "**1. Correlation Between Features and the Target (MedHouseVal)**\n",
    "\n",
    "- **Strong Correlation**: \n",
    "  - **MedInc (Median Income)** has a strong positive correlation with **MedHouseVal** (**0.69**). \n",
    "    - This means that as median income increases, house values tend to increase as well.\n",
    "    - This aligns with the economic principle that wealthier areas typically have higher property values.\n",
    "\n",
    "- **Moderate Correlations**:\n",
    "  - **AveRooms (Average Rooms per Dwelling)** has a moderate positive correlation (**0.33**) with **MedHouseVal**.\n",
    "    - More rooms per house might indicate larger or more luxurious properties, which could drive up house prices.\n",
    "\n",
    "- **Weak Correlations**:\n",
    "  - **HouseAge (Age of Houses)** shows a weak positive correlation (**0.11**) with **MedHouseVal**.\n",
    "    - This suggests that older houses might be slightly more valuable in the dataset's context, but the relationship isn't strong.\n",
    "  - **Latitude** shows a slightly negative correlation (**-0.14**) with **MedHouseVal**, which may indicate a trend where houses further north in the region are slightly less expensive. \n",
    "  - **AveBedrms (-0.10)**\n",
    "- **Negligible Correlations**: \n",
    "  - **Population (-0.03)**, **AveOccup (-0.02)**, and **Longitude (-0.05)** show very weak correlations with **MedHouseVal**. These features do not have significant predictive power for house prices on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2. Multicollinearity Insights**\n",
    "\n",
    "- **Longitude and Latitude (-0.93)**:\n",
    "  - This remains a strong negative correlation, as seen before. However, neither **Longitude** nor **Latitude** has a strong individual correlation with the target variable (**MedHouseVal**).\n",
    "  - This suggests that while geographic location matters, its direct influence on house prices is likely more complex and requires combining these two features (e.g., via clustering or distance to a specific location).\n",
    "\n",
    "- **AveRooms and AveBedrms (0.23)**:\n",
    "  - This is a weak positive correlation, meaning these features are loosely related. However, **AveRooms** shows a better relationship with target **MedHouseVal** than **AveBedrms**, so you might prefer to prioritize **AveRooms** in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion and Next Steps**\n",
    "\n",
    "- **Feature Selection**:\n",
    "    - **MedInc (Median Income)** is clearly the most influential feature for predicting house prices.\n",
    "    - **AveRooms**, **HouseAge** and **Latitude** might add additional predictive power due to their moderate correlations with the target.\n",
    "    - So, we'll select `MedInc`, `AveRooms`, and `HouseAge` as our primary features for modeling.\n",
    "\n",
    "- **Feature Reduction**:\n",
    "    - Due to the correlation between **AveRooms** and **AveBedrms**, one of these features might be redundant. Since **AveRooms** correlates better with the target, it may be wise to focus on it and drop **AveBedrms**.\n",
    "    - Features like **Population**, **AveOccup**, **AveBedrms**, and **Longitude** appear to have weaker direct relationships with house prices. So we might consider dropping them to reduce our feature space. \n",
    "\n",
    "- **Feature Creation**\n",
    "    - While neither of geographic features **Latitude** and **Longitude** show strong correlation with house prices individually, their **combined influence** might be significant. For example, a new feature based on the **distance to a central point**, such as the city center or a key landmark.\n",
    "\n",
    "- **Feature Transformation:** We'll be using normalizaiton technique to scale the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"MedInc\", \"HouseAge\", \"AveRooms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot\n",
    "**Scatter Plot** is a good way to visualize the data. We can show the relationship between two features and a feature with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_scatter(X, y):\n",
    "    data_combined = X.copy()\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    sns.pairplot(\n",
    "        data_combined,\n",
    "        y_vars=\"MedHouseVal\",\n",
    "        x_vars=X.columns,\n",
    "        height=5,\n",
    "        aspect=0.7,\n",
    "        kind=\"scatter\",\n",
    "    )\n",
    "\n",
    "\n",
    "plot_feature_target_scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the scatter plot of each feature against the target variable, we can see the relationship between the features and the target variable. This can help us understand the data better and make decisions about which features to include in the model or what kind of transformation we might need to do on the features.\n",
    "\n",
    "Looking at the scatter plot confirms our previous analysis regarding the `AveOccup` feature. Not only it has a very weak correlation with the target variable, but also the scatter plot shows that \"AveOccup\" has a heavy concentration at very low values without a proper distribution, as it lacks a balanced spread across its range and exhibits sparse, extreme outliers. Compare this to other features like `MedInc` and `AveRooms` which have ore uniform distribution across their range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_target_combined_scatter(X, y):\n",
    "    data_combined = X.copy()\n",
    "    # Add the target to the data to plot the combined scatter plot\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    sns.pairplot(data_combined)\n",
    "\n",
    "\n",
    "plot_features_target_combined_scatter(X[selected_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above pairplot shows the scatter plot of each feature against the target variable. The diagonal plots are **histograms** of the features. Histograms show the distribution of a feature across its range. For example, the `AveRooms` shows the higher number of houses with around 5 rooms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Insights and the next steps:**\n",
    "- The plot shows a good spread of data points among the features, and the relationship between the features and the target variable. It also shows a good spread of feature values across their range (in the Histograms). This is a good indication that the selected features are good candidates for the model.\n",
    "- In the next step, we'll normalize the features to have a similar scale before training the model. Although we have a good spread of data points, the features have different scales. For example, `HouseAge` ranges from 1 to 52, while `AveRooms` ranges from 0.84 to 10. This can cause issues in the model training process, as the model might give more weight (importance) to the features with higher values. So, we'll [normalize](../feature_engineering.md#scaling-and-normalization) the features to have a similar scale before training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of selected features\n",
    "print(X[selected_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used `seaborn` library. `seaborn` is built on top of `matplotlib` and provides a high-level interface for creating attractive and informative statistical graphics. However, we can achive the same by using `matplotlib` library directly.\n",
    "\n",
    "Let's see the scatter plot of `MedInc` and `HouseAge` features against the target variable `MedHouseVal` using `Matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for 'MedInc' vs 'MedHouseVal'\n",
    "def plot_feature_target_matplotlib(X, y):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[\"MedInc\"], y, alpha=0.5)\n",
    "    plt.title(\"Median Income vs Median House Value\")\n",
    "    plt.xlabel(\"Median Income\")\n",
    "    plt.ylabel(\"Median House Value\")\n",
    "\n",
    "    # Scatter plot for 'HouseAge' vs 'MedHouseVal'\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[\"HouseAge\"], y, alpha=0.5)\n",
    "    plt.title(\"House Age vs Median House Value\")\n",
    "    plt.xlabel(\"House Age\")\n",
    "    plt.ylabel(\"Median House Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature_target_matplotlib(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features\n",
    "As we discussed in the previous steps, we want to create a new feature based on the `Latitude` and `Longitude` features. We can create a new feature called `Distance` which in this exaple is the distance of each house from the centre of California with the coordinates (37.16611, -119.44944)`.\n",
    "\n",
    "> Depending on the domain and data, we can create new features in different ways. For example, we can create a distance of each house from landmark from the ocean as the house price tends to be higher closer to the ocean. However, for simpliciy we'll use the distance from the centre of California.\n",
    "\n",
    "\n",
    "**Distance Calculation**:\n",
    "\n",
    "To calculate the distances with high accuracy we need to consider the curvature of the Earth. The _Haversine formula_ is a good choice for calculating the distance between two points on the Earth's surface. However, in this example for simplicity, we'll use the Eucledian distance formula which is good for small distances.\n",
    "\n",
    "The Eucledian distance between two points $(x_1, y_1)$ and $(x_2, y_2)$ is calculated as:\n",
    "$$ \\text{Distance} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    # Scale factor, 111 km per degree latitude\n",
    "    lat_scale = 111\n",
    "\n",
    "    # km per degree longitude at given latitude\n",
    "    lon_scale = 111 * np.cos(np.radians(lat1))\n",
    "    return np.sqrt(\n",
    "        ((lat2 - lat1) * lat_scale) ** 2 + ((lon2 - lon1) * lon_scale) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The scaling factor of `111` comes from the Earth's geometry and is used to approximate the conversion of degrees of latitude or longitude into kilometers.\n",
    "\n",
    "Now let's apply this formula to calculate the distance of each house from the centre of California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the scaled Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "In the final step that we are happy with the selection of feature and added any new features, we want to [normalize](../feature_engineering.md#scaling-and-normalization) the values of these features to have a similar scale. We'll use `StandardScaler` from `sklearn.preprocessing` module to normalize the features. `StandardScaler` uses the [z-score](../feature_engineering.md#z-score-normalization) normalization method to normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(X[selected_features]), columns=selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should we scale both X and y?**\n",
    "\n",
    "Typically we normalize `y` in **regression problems** (where the target variable is continuous) or with scale-sensitive models such as Neural Networks.\n",
    "However, we don't normalize `y` in **classification problems** (where the target variable is categorical) or dealing with scale-insensitive models such as Decision Trees, Random Forests, or Gradient Boosting models.\n",
    "\n",
    "So, since here we are dealing with a regression problem, we'll normalize the target variable `y` as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the shape of `y` from `(m,)` to `(m, 1)` to be able to use `StandardScaler` on it. In other words, we need to convert `y` from a vector to a matrix with one column and as many rows as needed to accommodate all the data. `reshape(-1, 1)` will change the shape of this array to have as many rows as needed and one column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_normalized = scaler.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_target_combined_scatter(X_normalized, y_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
