{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use:\n",
    "- `scikit-learn` library to train a linear regression model\n",
    "- `seaborn` and `matplotlib` libraries to visualize the data and the model\n",
    "- California Housing dataset from `scikit-learn` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in [ML Workflow](../ml_workflow.md), there are key steps before we are ready for modelling. Those steps are data engineering, EPA (Exploratory Data Analysis), and feature engineering. In this simple example, we touch on those steps breifly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Engineering\n",
    "\n",
    "In this example, our training dataset is reading to use from the `sklearn.datasets` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "In this example, the data engineering steps (collect, cleanse and preprocess the data) are already done for us. So, we simply use the California Housing dataset from `sklearn.datasets` module. This dataset has around 20,000 samples and 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset.\n",
    "housing_data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) and Feature Engineering\n",
    "\n",
    "We will briefly [explore and visualize](../ml_workflow.md#visualization-techniques)   the data and their relationship. Also, we will briefly touch on common steps of _Feature Selection_, _Feature Creation_ and _Feature Scaling_. \n",
    "\n",
    "Let's start getting familiar with our data.  The goal of this step is to first understand our data, and then to engineer the features in a way that our dataset is ready for training a high performing model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape and Types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the type and shapes of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the input features.\n",
    "X = housing_data.data\n",
    "\n",
    "# y is the target/label\n",
    "y = housing_data.target\n",
    "\n",
    "print(f\"X type: {type(X)}, X shape: {X.shape}\")\n",
    "print(f\"y type: {type(y)}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is a matrix of `m` samples and `n` features. Here `m = 20640` and `n = 8`. So, we have 8 features and total of 20640 samples.\n",
    "\n",
    "`y` is a vector of `m` samples. So, since here we are doing a _superivsed learning_ task, we should have target values for each of the samples. So, `y` is a vector of 20640 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X values:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\ny values in $100,000:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect each of the examples individually by using `pandas` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first row of X and its target y values:\n",
    "print(\"\\nFirst row of X and it's target value y:\")\n",
    "print(\"X_1:\")\n",
    "print(X.iloc[0])\n",
    "print(\"y_1:\")\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Outliers\n",
    "Now let's explore our data to check for any outliers. Detecting outliers is important as they can have a disproportionate effect on the model and lean to poor performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of each feature\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we already can see some outliers. For example, the `AveRooms` feature has some very high values e.g. 141 which is probably an outlier for a typical housing dataset. Or the number of occupants per household `AveOccup` has some very high values e.g. 1243. Those are probably outliers.\n",
    "\n",
    "However before we decide to remove them, let's visualize the data and see if we can get more insights. We'll use `seaborn` and `matplotlib` libraries for visualization. \n",
    "\n",
    "> Note: EDA and Feature Engineering is a process that needs to be done carefully and iteratively. This process needs a lot of domain knowledge and experience. For example, a valid but rare case might be considered as an outlier, etc.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box Plot**  \n",
    "A box plot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the box plot for one of the features `AveRooms`. Depending on the domain and the data, we can decide which features to start with for the clean up process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=X[\"AveRooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplot:\n",
    "- **Top Line (Upper Whisker)**: The top line represents the Q3, which the value below which 75% of the data falls.\n",
    "- **Middle Line**: The middle box also called **Interquartile Range (IQR)** which represents the median, which is the middle value of the dataset. $IQR = Q3 - Q1$\n",
    "- **Bottom Line (Lower Whisker)**: The bottom line represents the Q1, which the value below which 25% of the data falls. \n",
    "\n",
    "**Beyond the Whiskers**: The small circles are the indicator of potential outliers, as they fall beyond the acceptable range of typical data variability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the outliers\n",
    "We can start by determining the range of acceptable values using the **IQR (Interquartile Range)** method, which is standard for identifying outliers. However, we can apply a **softer approach** by increasing the threshold multiplier (e.g., $k = 2.5$ instead of the usual $1.5$) to only exclude extreme outliers. \n",
    "\n",
    "\n",
    "1. **Compute IQR:**\n",
    "   - Calculate $Q1$ (25th percentile) and $Q3$ (75th percentile) of `AveRooms`.\n",
    "   - Compute the IQR: $ \\text{IQR} = Q3 - Q1 $.\n",
    "   - Define thresholds:\n",
    "    $$ \\text{Lower Bound} = Q1 - k \\times \\text{IQR} $$\n",
    "    $$ \\text{Upper Bound} = Q3 + k \\times \\text{IQR} $$\n",
    "     We'll start with a **softer multiplier** like $k = 2.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute Q1, Q3, and IQR\n",
    "Q1 = X[\"AveRooms\"].quantile(0.25)\n",
    "Q3 = X[\"AveRooms\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds (using a softer threshold with k = 2.5)\n",
    "k = 2.5\n",
    "lower_bound = Q1 - k * IQR\n",
    "upper_bound = Q3 + k * IQR\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "print(f\"Lower bound: {lower_bound}\")\n",
    "print(f\"Upper bound: {upper_bound}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Filter Out Outliers:**\n",
    "   - Remove rows where the `AveRooms` value is outside the bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter the dataset based on the bounds\n",
    "cleaned_X = X[\n",
    "    (X[\"AveRooms\"] >= lower_bound) & (X[\"AveRooms\"] <= upper_bound)\n",
    "].copy()\n",
    "\n",
    "# Filter the target variable based on the filtered features\n",
    "cleaned_y = y[cleaned_X.index].copy()\n",
    "\n",
    "print(f\"Original dataset size: X={X.shape[0]}, y={y.shape[0]}\")\n",
    "print(f\"Cleaned dataset size: X={cleaned_X.shape[0]}, y={cleaned_y.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned up removed a small fraction of the data, which is a good sign that the data is not too noisy, and we probably removed the outliers, not the valid data.\n",
    "\n",
    "3. **Validate:**\n",
    "   - Visualize the boxplot again to ensure the outliers are handled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=cleaned_X[\"AveRooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is a better plot now. We can see the obvious outliers are removed. You can continue this by adjusting the threshold of $k$ to lower or higher values and see how it affects the data.\n",
    "\n",
    "We can repeat the same process for other features as well and clean up the outliers. Although, we may decide to have a light clean up first, then go through the next steps which help us decide about the feature selection. Then after knowing the chosen features, we can come back to this step and do a more thorough clean up.\n",
    "\n",
    "> Note: EDA and Feature Engineering (similar to the whole ML process) is an **empirical process** and needs to be done **iteratively**. We may need to come back to this step after the feature selection and engineering steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    "This is a useful way to visualize the relationship between features (and features-target) which can help us in feature selection. This view in particular is useful to detect multicollinearity (when two or more features are highly correlated, i.e move of one, impact the other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation**    \n",
    " means that those two features are moving together. Correlation values range from -1 to 1. For example:\n",
    "- Positive correlation is when one feature increases, the other feature also increases.\n",
    "- Negative correlation is when one feature increases, the other feature decreases.\n",
    "\n",
    "As this influence from a feature to another increases, the correlation value moves towards 1 or -1. For example, two features with a correlation value of $0.7$ are much more correlated than two features with a correlation value of $0.2$. Similarly, two features with a correlation value of $-0.7$ are much more negatively correlated than two features with a correlation value of $-0.2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-collinearity Detection**  \n",
    "A common use of heatmap is to detect **multi-collinearity** between features. Multi-collinearity is a phenomenon in which two or more features in a dataset are highly correlated. This can cause problems in the model, such as unstable coefficients, and it can make it difficult to determine the effect of each feature on the target variable. In which those cases, we either:\n",
    "- Remove one of the features (Feature Selection).\n",
    "- Combine the features and create a new feature from them (New Feature Creation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(X, y):\n",
    "    data_combined = X.copy()\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    corr = data_combined.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(cleaned_X, cleaned_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following guide for interpreting correlation thresholds: \n",
    "\n",
    "- Negligible: $|r| < 0.1$ (often ignored)\n",
    "- Weak: $0.1 \\leq |r| < 0.3$\n",
    "- Moderate: $0.3 \\leq |r| < 0.5$\n",
    "- Strong: $0.5 \\leq |r| < 0.7$\n",
    "- Very Strong: $|r| \\geq 0.7$\n",
    "\n",
    "For feature selection, prioritize $|r| \\geq 0.3$, and focus on $|r| \\geq 0.5$ for strong relationships unless domain knowledge dictates otherwise.\n",
    "\n",
    "> The above is not a strict rule, but a guideline which can be adjusted based on the domain knowledge and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Analyze this heatmap: \n",
    "\n",
    "**1. Correlation Between Features and the Target (MedHouseVal)**\n",
    "\n",
    "- **Strong Correlation**: \n",
    "  - **MedInc (Median Income)** has a strong positive correlation with **MedHouseVal** (**0.69**). \n",
    "    - This means that as median income increases, house values tend to increase as well.\n",
    "    - This aligns with the economic principle that wealthier areas typically have higher property values.\n",
    "\n",
    "- **Moderate Correlations**:\n",
    "  - **AveRooms (Average Rooms per Dwelling)** has a moderate positive correlation (**0.33**) with **MedHouseVal**.\n",
    "    - More rooms per house might indicate larger or more luxurious properties, which could drive up house prices.\n",
    "\n",
    "- **Weak Correlations**:\n",
    "  - **HouseAge (Age of Houses)** shows a weak positive correlation (**0.11**) with **MedHouseVal**.\n",
    "    - This suggests that older houses might be slightly more valuable in the dataset's context, but the relationship isn't strong.\n",
    "  - **Latitude** shows a slightly negative correlation (**-0.14**) with **MedHouseVal**, which may indicate a trend where houses further north in the region are slightly less expensive. \n",
    "  - **AveBedrms (-0.10)**\n",
    "- **Negligible Correlations**: \n",
    "  - **Population (-0.03)**, **AveOccup (-0.02)**, and **Longitude (-0.05)** show very weak correlations with **MedHouseVal**. These features do not have significant predictive power for house prices on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Multicollinearity Insights**\n",
    "\n",
    "- **Longitude and Latitude (-0.93)**:\n",
    "  - This is a strong negative correlation. However, neither **Longitude** nor **Latitude** has a strong individual correlation with the target variable (**MedHouseVal**). Although since our dataset is based on California housing, and California state is spread vertically more than horizontally, Latitude (which is a horizontal lines) has a stronger correlation with the target than Longitude. This suggest that in our dataset the influence of location of the house between south and north of the state is more important than the influence of the location between east and west of the state.\n",
    "\n",
    "  - There are multiple options to handle this:\n",
    "    - **Feature Selection**: Keep only one of the features. (e.g., **Latitude** which has a larger correlation with the target).\n",
    "    - **New Feature Using PCA**: Use [Principal Component Analysis (PCA)](../feature_engineering.md#pca-principal-component-analysis) to reduce the dimensionality of the dataset. This will create new features that are linear combinations of the original features, which can help reduce multicollinearity.\n",
    "    - **New Feature by Calculating Distance**: Create a new feature that combines both features (e.g., distance to a specific location). \n",
    "\n",
    "- **AveRooms and AveBedrms (0.23)**:\n",
    "  - This is a weak positive correlation, meaning these features are loosely related. However, **AveRooms** shows a better relationship with target **MedHouseVal** than **AveBedrms**, so you might prefer to prioritize **AveRooms** in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion and Next Steps**\n",
    "\n",
    "- **Feature Selection**:\n",
    "    - **MedInc (Median Income)** is clearly the most influential feature for predicting house prices.\n",
    "    - **AveRooms**, **HouseAge** and **Latitude** might add additional predictive power due to their moderate correlations with the target.\n",
    "    - Note: there are other techniques for feature selection such as [**Univariate Statistical Tests**](https://scikit-learn.org/1.5/modules/feature_selection.html#univariate-feature-selection), **Recursive Feature Elimination (RFE)**, **Forward Selection**, **Backward Elimination**, etc. In this example, to go through the analysis step by step and for simplicity, we'll manually select the features based on our analysis.\n",
    "\n",
    "- **Feature Reduction**:\n",
    "    - Due to the correlation between **AveRooms** and **AveBedrms**, one of these features might be redundant. Since **AveRooms** correlates better with the target, it may be wise to focus on it and drop **AveBedrms**.\n",
    "    - Features like **Population**, **AveOccup**, **AveBedrms**, and **Longitude** appear to have weaker direct relationships with house prices. So we might consider dropping them to reduce our feature space. \n",
    "\n",
    "- **Feature Creation**\n",
    "    - While neither of geographic features **Latitude** and **Longitude** show strong correlation with house prices individually, their **combined influence** might be significant. For example, a new feature based on the **distance to a central point**, such as the city center or a key landmark. However, we won't create new features in this example.\n",
    "\n",
    "- **Feature Transformation:** We'll be using normalizaiton technique to scale the features into a similar range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"Latitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `scikit-learn` built-in **Univariate Feature Selection** method to select the top 4 features and compare that with our manual selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Select top 4 features for regression\n",
    "selector = SelectKBest(score_func=f_regression, k=4)\n",
    "selector.fit_transform(X, y)\n",
    "print(\"Selected features:\", X.columns[selector.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seems we are on the right track with our manual selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot\n",
    "**Scatter Plot** is a good way to visualize the data. We can show the relationship between two features and a feature with the target variable.\n",
    "\n",
    "The scatter plot of each feature against the target variable, shows the relationship between that feature and the target variable. This can help us understand the data and how spread out the feature/target values are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_scatter(X, y):\n",
    "    data_combined = X.copy()\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    sns.pairplot(\n",
    "        data_combined,\n",
    "        y_vars=\"MedHouseVal\",\n",
    "        x_vars=X.columns,\n",
    "        height=5,\n",
    "        aspect=0.7,\n",
    "        kind=\"scatter\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at the scatter plots for the selected features **before** removing the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_scatter(X[selected_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above scatter plot, we can see a `AveRooms` feature has a heavy concentration at very low values without a proper distribution, as it lacks a balanced spread across its range and exhibits sparse, extreme outliers. \n",
    "\n",
    "Compare this now with the distribution after removing the outliers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_scatter(cleaned_X[selected_features], cleaned_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot looks much better now. The data is more spread out and the outliers are removed. This will help the model to learn better and generalize better on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Matrix\n",
    "A scatter plot matrix is a grid of scatter plots that shows the relationship between every pair of features in a dataset. The diagonal of the matrix shows the distribution of a single feature, while the other cells show the relationship between two features (and the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_target_scatter_matrix(X, y):\n",
    "    data_combined = X.copy()\n",
    "    # Add the target to the data to plot the combined scatter plot\n",
    "    data_combined[\"MedHouseVal\"] = y\n",
    "    sns.pairplot(data_combined)\n",
    "\n",
    "\n",
    "plot_features_target_scatter_matrix(cleaned_X[selected_features], cleaned_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights and the next steps:**\n",
    "- The above pairplot shows the scatter plot of each feature against the target variable. The diagonal plots are **histograms** of the features. Histograms show the distribution of a feature across its range. For example, the `AveRooms` shows the higher number of houses with around 5 rooms. \n",
    "\n",
    "\n",
    "- The plot shows a good spread of data points among the features, and the relationship between the features and the target variable. It also shows a good spread of feature values across their range (in the Histograms). This is a good indication that the selected features are good candidates for the model without significant outliers and imbalances of scale. Although we can see that features still have different scales. For example, `HouseAge` ranges from 1 to 52, while `AveRooms` ranges from 0.84 to 10. This can cause issues in the model training process, as the model might give more weight (importance) to the features with higher values. So, at the end, we'll [normalize](../feature_engineering.md#scaling-and-normalization) the features to have a similar scale before training the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used `seaborn` library. `seaborn` is built on top of `matplotlib` and provides a high-level interface for creating attractive and informative statistical graphics. However, we can achive the same by using `matplotlib` library directly.\n",
    "\n",
    "Let's see the scatter plot of `MedInc` and `HouseAge` features against the target variable `MedHouseVal` using `Matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_matplotlib(X, y):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[\"MedInc\"], y, alpha=0.5)\n",
    "    plt.title(\"Median Income vs Median House Value\")\n",
    "    plt.xlabel(\"Median Income\")\n",
    "    plt.ylabel(\"Median House Value\")\n",
    "\n",
    "    # Scatter plot for 'HouseAge' vs 'MedHouseVal'\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[\"HouseAge\"], y, alpha=0.5)\n",
    "    plt.title(\"House Age vs Median House Value\")\n",
    "    plt.xlabel(\"House Age\")\n",
    "    plt.ylabel(\"Median House Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature_target_matplotlib(cleaned_X, cleaned_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "When we are happy with the selection of our features and added any new features (if any), it's the time to to [normalize](../feature_engineering.md#scaling-and-normalization) the values of these features to have a similar scale. We'll use `StandardScaler` from `sklearn.preprocessing` module to normalize the features. `StandardScaler` uses the [z-score](../feature_engineering.md#z-score-normalization) normalization method to normalize the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the current state of our data with each feature in different ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of selected features\n",
    "print(cleaned_X[selected_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_X = pd.DataFrame(\n",
    "    scaler.fit_transform(cleaned_X[selected_features]),\n",
    "    columns=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should we scale both X and y?**\n",
    "\n",
    "Typically we normalize `y` in **regression problems** (where the target variable is continuous) or with scale-sensitive models such as Neural Networks.\n",
    "However, we don't normalize `y` in **classification problems** (where the target variable is categorical) or dealing with scale-insensitive models such as Decision Trees, Random Forests, or Gradient Boosting models.\n",
    "\n",
    "So, since here we are dealing with a regression problem, we'll normalize the target variable `y` as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the shape of `y` from `(m,)` to `(m, 1)` to be able to use `StandardScaler` on it. In other words, we need to convert `y` from a vector to a matrix with one column and as many rows as needed to accommodate all the data. `reshape(-1, 1)` will change the shape of this array to have as many rows as needed and one column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_y = scaler.fit_transform(cleaned_y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "We are now ready to train our model. We'll use [**Gradient Descent**](../linear_regression.md#gradient-descent) to train a linear regression model.\n",
    "\n",
    "In the following steps we'll:\n",
    "\n",
    "- Divide the dataset into training and testing sets.\n",
    "- Use [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#examples-using-sklearn-linear-model-sgdregressor) which is an implementation of [Stochastic Gradient Descent](../gradient_descent.md#stochastic-gradient-descent-sgd) algorithm.\n",
    "- Evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "We'll use 20% of the data for testing and 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 20% test data, 80% training data\n",
    "test_size = 0.2\n",
    "# Set a seed for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    normalized_X, normalized_y, test_size=test_size, random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random_state` is used to ensure that the split is the same each time we run the code. This is useful for reproducibility and debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "We'll train the model with 1000 epochs (iterations) and a learning rate of 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "sgd_regressor = SGDRegressor(max_iter=epochs, alpha=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_regressor.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ravel()` function is used to convert the target variable `y` from a column-vector to a 1d array which is expected by the SGDRegressor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\", sgd_regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of iterations completed: {sgd_regressor.n_iter_}\")\n",
    "print(f\"Number of weight updates: {sgd_regressor.t_}\")\n",
    "print(\"Stopping criteria: tol =\", sgd_regressor.tol)\n",
    "print(\"n_iter_no_change =\", sgd_regressor.n_iter_no_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why the number of iterations completed is less than the number of max epoch?\n",
    "\n",
    "    The default values in SGDRegressor:\n",
    "    ```\n",
    "    max_iter=1000\n",
    "    tol=1e-3\n",
    "    n_iter_no_change=5\n",
    "    ```\n",
    "\n",
    "    This means that the training will stop if the improvement in the loss function is smaller than `0.001` for `5` consecutive epochs, or after `1000` epochs, whichever comes first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How the number of weight updates is calculated?  \n",
    "    The number of weight updates is equal to the number of samples multiplied by the number of epochs. \n",
    "\n",
    "    `number of weight updates = m * n_iter_`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
