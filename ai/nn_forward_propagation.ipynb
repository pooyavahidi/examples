{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Forward Propagation\n",
    "We'll implement a simple forward pass of the neural network with 2 fully connected layers and Sigmoid activation function. First, we'll use PyTorch to implement the forward pass and then we'll implement the matrix multiplication and activation function from scratch to understand the working of the neural network.\n",
    "\n",
    "For more details see [Neural Networks - Inference (Forward Propagation)](https://github.com/pooyavahidi/content/blob/main/ai/neural_networks_inference.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass using PyTorch\n",
    "Let's implement a simple forward pass of a neural network with 2 fully connected layers using PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features=2, out_features=3)\n",
    "        self.linear2 = torch.nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z1 = self.linear1(x)\n",
    "        print(f\"Z1: {Z1.data}\")\n",
    "\n",
    "        A1 = F.sigmoid(Z1)\n",
    "        print(f\"A1: {A1.data}\")\n",
    "\n",
    "        Z2 = self.linear2(A1)\n",
    "        print(f\"Z2: {Z2.data}\")\n",
    "\n",
    "        A2 = F.sigmoid(Z2)\n",
    "        print(f\"A2: {A2.data}\")\n",
    "\n",
    "        return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input dataset with 3 examples (batch size = 2) with 2 features each. So, $X$ is with shape of $(3, 2)$.\n",
    "\n",
    "\n",
    "The matrix of input features is defined like all other input feature matrix with each row represent a **feature** and each column represent an **example**. Input feature shape is `(number of examples/batch size, number of features)`.\n",
    "\n",
    "$$X =  \\begin{bmatrix} \\vec{\\mathbf{x}}^{(1)} \\\\\n",
    "\\vec{\\mathbf{x}}^{(2)} \\\\\n",
    "\\cdots \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here we have 3 examples, each with 2 features. Example 1 has features $x_1^{(1)} = 0.25$ and $x_2^{(1)} = -0.45$, and so on:\n",
    "\n",
    "$$X =  \\begin{bmatrix} 1.25 & 0.38 \\\\\n",
    "-0.45 & 3.01 \\\\\n",
    "0.72 & -0.56 \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3 examples (Batch size = 3):\n",
    "# Example 1: (Feature 1 = 1.25, Feature 2 = 0.38)\n",
    "# Example 2: (Feature 1 = -0.45, Feature 2 = 3.01)\n",
    "# Example 3: (Feature 1 = 0.72, Feature 2 = -0.56)\n",
    "X = np.array([[1.25, 0.38], [-0.45, 3.01], [0.72, -0.56]])\n",
    "print(f\"X Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix $W$ of a layer is with the shape of $(n^{[l]}, n^{[l-1]})$ where each row represents the weights of a single neuron in the layer. In other words, $W$ is the dimension of $(output \\times input)$ which output is the number of neurons in the layer and input is the number of neurons in the previous layer.\n",
    "\n",
    "$b$ is a vector of size number of $neurons$ in the layer, one bias for each neuron.\n",
    "\n",
    "- Layer 0 (input layer)  with 2 features. \n",
    "- Layer 1 with 3 neurons (in = 2, out = 3): $W_1$ is $(3, 2)$ and $b_1$ is $(3,)$\n",
    "- Layer 2 with 1 neuron  (in = 3, out = 1): $W_2$ is $(1, 3)$ and $b_2$ is $(1,)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "W1 = np.array([[-0.6053, 0.2325], [-0.5255, -0.6182], [0.0117, -0.1774]])\n",
    "b1 = np.array([0.3849, -0.6344, -0.2022])\n",
    "\n",
    "# Layer 2 (Output Layer)\n",
    "W2 = np.array([[0.3884, -0.4516, -0.0486]])\n",
    "b2 = np.array([0.4796])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the parameters of the model manually for repeatability. Then, we'll use the same values in the manual implementation to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Weights: torch.Size([3, 2]), Bias: torch.Size([3])\n",
      "Layer 2: Weights: torch.Size([1, 3]), Bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model.linear1.weight.data.copy_(torch.tensor(W1))\n",
    "model.linear1.bias.data.copy_(torch.tensor(b1))\n",
    "\n",
    "model.linear2.weight.data.copy_(torch.tensor(W2))\n",
    "model.linear2.bias.data.copy_(torch.tensor(b2))\n",
    "\n",
    "print(\n",
    "    f\"Layer 1: Weights: {model.linear1.weight.data.shape}, \"\n",
    "    f\"Bias: {model.linear1.bias.data.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Layer 2: Weights: {model.linear2.weight.data.shape}, \"\n",
    "    f\"Bias: {model.linear2.bias.data.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the values of the weights and biases of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: \n",
      "--------------------\n",
      "Weights:\n",
      "tensor([[-0.6053,  0.2325],\n",
      "        [-0.5255, -0.6182],\n",
      "        [ 0.0117, -0.1774]])\n",
      "Bias:\n",
      "tensor([ 0.3849, -0.6344, -0.2022])\n",
      "\n",
      "Layer 2: \n",
      "--------------------\n",
      "Weights:\n",
      "tensor([[ 0.3884, -0.4516, -0.0486]])\n",
      "Bias:\n",
      "tensor([0.4796])\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1: \")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Weights:\\n{model.linear1.weight.data}\")\n",
    "print(f\"Bias:\\n{model.linear1.bias.data}\")\n",
    "\n",
    "print(\"\\nLayer 2: \")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Weights:\\n{model.linear2.weight.data}\")\n",
    "print(f\"Bias:\\n{model.linear2.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning frameworks (such as TensorFlow and PyTorch), keep the bias as a vector (1D array) for efficiency, but for matrix multiplicatin automatically broadcasts to effectively behave like a matrix with shape of $(n,1)$ when needed.\n",
    "\n",
    "Vector $b$ is broadcasted to the shape of the matrix $Z$ during the addition operation. So, in this case, it will converted to a row vector of size $(1, n^{[l]})$ and added to each row of the matrix $Z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1: tensor([[-0.2834, -1.5262, -0.2550],\n",
      "        [ 1.3571, -2.2587, -0.7414],\n",
      "        [-0.1811, -0.6666, -0.0944]])\n",
      "A1: tensor([[0.4296, 0.1786, 0.4366],\n",
      "        [0.7953, 0.0946, 0.3227],\n",
      "        [0.4548, 0.3393, 0.4764]])\n",
      "Z2: tensor([[0.5446],\n",
      "        [0.7301],\n",
      "        [0.4799]])\n",
      "A2: tensor([[0.6329],\n",
      "        [0.6748],\n",
      "        [0.6177]])\n",
      "tensor([[0.6329],\n",
      "        [0.6748],\n",
      "        [0.6177]])\n"
     ]
    }
   ],
   "source": [
    "y_pred_torch = model(torch.tensor(X, dtype=torch.float32))\n",
    "print(y_pred_torch.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass using Matrix Multiplication (from scratch)\n",
    "Let's now go through layer by layer calculations and compare the result with the PyTorch implementation.\n",
    "\n",
    "We'll calculate the output of each layer using the following steps:\n",
    "\n",
    "**1. Linear Transformation for layer $l$**:<br>\n",
    "\n",
    "$$Z^{[1]} = A^{[l-1]}{W^{[l]}}^\\top + \\vec{\\mathbf{b}}^{[l]}$$\n",
    "\n",
    "**2. Activation Function for layer $l$**:<br>\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]})$$\n",
    "\n",
    "where $\\sigma$ is the activation function (Sigmoid in this case):\n",
    "\n",
    "$$g(Z) = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "More on this [here](https://github.com/pooyavahidi/content/blob/main/ai/neural_networks_inference.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function Definition**:<br>\n",
    "We'll use the Sigmoid activation function for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1\n",
    "\n",
    "The input matrix $X$ is with 3 examples.\n",
    "$$X =  \\begin{bmatrix} 1.25 & 0.38 \\\\\n",
    "-0.45 & 3.01 \\\\\n",
    "0.72 & -0.56 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "As we discussed input layer can also be referred to as layer 0.\n",
    "\n",
    "$$X = A^{[0]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformation for Layer 1\n",
    "\n",
    "$$Z^{[1]} = A^{[0]}{W^{[1]}}^\\top + \\vec{\\mathbf{b}}^{[1]}$$\n",
    "\n",
    "$$Z^{[1]} = \\begin{bmatrix} 1.25 & 0.38 \\\\\n",
    "-0.45 & 3.01 \\\\\n",
    "0.72 & -0.56 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix} -0.6053 & -0.5255 & 0.0117 \\\\\n",
    "0.2325 & -0.6182 & -0.1774 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix} 0.3849 & -0.6344 & -0.2022 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Which results in:\n",
    "\n",
    "$$= \\begin{bmatrix} 1.25 \\times -0.6053 + 0.38 \\times 0.2325 & 1.25 \\times -0.5255 + 0.38 \\times -0.6182 & 1.25 \\times 0.0117 + 0.38 \\times -0.1774 \\\\ \n",
    "-0.45 \\times -0.6053 + 3.01 \\times 0.2325 & -0.45 \\times -0.5255 + 3.01 \\times -0.6182 & -0.45 \\times 0.0117 + 3.01 \\times -0.1774 \\\\\n",
    "0.72 \\times -0.6053 + -0.56 \\times 0.2325 & 0.72 \\times -0.5255 + -0.56 \\times -0.6182 & 0.72 \\times 0.0117 + -0.56 \\times -0.1774 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix} 0.3849 & -0.6344 & -0.2022 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then we broadcast the bias vector to each row of the matrix:\n",
    "$$= \\begin{bmatrix} -0.668275 + 0.3849 & -0.891791 - 0.6344 & -0.052787 - 0.2022 \\\\\n",
    "0.97221 + 0.3849 & -1.624307 - 0.6344 & -0.539239 - 0.2022 \\\\\n",
    "-0.566016 + 0.3849 & -0.032168 - 0.6344 & 0.107768 - 0.2022 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Which then the final result is:\n",
    "\n",
    "$$Z^{[1]} = \\begin{bmatrix} -0.283375 & -1.526191 & -0.254987 \\\\\n",
    "1.35711 & -2.258707 & -0.741439 \\\\\n",
    "-0.181116 & -0.666568 & -0.094432 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Each row of the matrix $Z^{[1]}$ corresponds to a single example in the batch. We can interpret each row of this matrix as the the linear transformation of the a single example by the first layer of the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caclulate the above using matrix multiplication in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1: [[-0.283375 -1.526191 -0.254987]\n",
      " [ 1.35711  -2.258707 -0.741439]\n",
      " [-0.181116 -0.666568 -0.094432]]\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(X, W1.T) + b1\n",
    "print(f\"Z1: {Z1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function for Layer 1\n",
    "\n",
    "$$A^{[1]} = g(Z^{[1]})$$\n",
    "\n",
    "$$A^{[1]} = \\begin{bmatrix} \\sigma(-0.283375) & \\sigma(-1.526191) & \\sigma(-0.254987) \\\\\n",
    "\\sigma(1.35711) & \\sigma(-2.258707) & \\sigma(-0.741439) \\\\\n",
    "\\sigma(-0.181116) & \\sigma(-0.666568) & \\sigma(-0.094432) \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The result for the first column of the first row is:\n",
    "\n",
    "$$\\sigma(-0.283375) = \\frac{1}{1 + e^{-(-0.283375)}} = 0.42962654$$\n",
    "\n",
    "If we calculate the rest of the values, we get:\n",
    "\n",
    "$$A^{[1]} = \\begin{bmatrix} 0.42962654 & 0.17855167 & 0.43659641 \\\\\n",
    "0.7952896 & 0.09460106 & 0.32268955 \\\\\n",
    "0.45484437 & 0.33926575 & 0.47640953 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix $A^{[1]}$ is the output of the first layer.\n",
    "- Each row of the matrix $A^{[1]}$ corresponds to a single example in the batch. We can interpret each row of this matrix as the output vector $\\vec{\\mathbf{a}}^{(i)}$ for a single example. \n",
    "- Each column is a activation value of a neuron in the previous layer for all examples. We have 3 neurons in the first layer, so we have 3 columns in the output matrix.\n",
    "\n",
    "For example in the first row:\n",
    "\n",
    "$${\\vec{\\mathbf{a}}^{[1]}}^{(1)} = \\begin{bmatrix} 0.42962654 & 0.17855167 & 0.43659641 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where:\n",
    "- $[l]$ is the layer index.\n",
    "- $(i)$ is the example index.\n",
    "\n",
    "So:\n",
    "- ${a^{[1]}_{1}}^{(1)} = 0.42962654$ is the output of the first neuron in the first layer for the first example.\n",
    "- ${a^{[1]}_{2}}^{(1)} = 0.17855167$ is the output of the second neuron in the first layer for the first example.\n",
    "- ${a^{[1]}_{3}}^{(1)} = 0.43659641$ is the output of the third neuron in the first layer for the first example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the above using numpy and compare the results with the PyTorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1: [[0.42962654 0.17855167 0.43659641]\n",
      " [0.7952896  0.09460106 0.32268955]\n",
      " [0.45484437 0.33926575 0.47640953]]\n"
     ]
    }
   ],
   "source": [
    "A1 = sigmoid(Z1)\n",
    "print(f\"A1: {A1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2\n",
    "\n",
    "Now to calculate the output of the second layer, we need to use the output of the first layer as the input to this layer.\n",
    "\n",
    "Input:\n",
    "\n",
    "$$A^{[1]} =  \\begin{bmatrix} 0.42962654 & 0.17855167 & 0.43659641 \\\\\n",
    "0.7952896 & 0.09460106 & 0.32268955 \\\\\n",
    "0.45484437 & 0.33926575 & 0.47640953 \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformation for Layer 2\n",
    "\n",
    "$$Z^{[2]} = A^{[1]}{W^{[2]}}^\\top + \\vec{\\mathbf{b}}^{[2]}$$\n",
    "\n",
    "$$Z^{[2]} = \\begin{bmatrix} 0.42962654 & 0.17855167 & 0.43659641 \\\\\n",
    "0.7952896 & 0.09460106 & 0.32268955 \\\\\n",
    "0.45484437 & 0.33926575 & 0.47640953 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix} 0.3884 \\\\\n",
    "-0.4516 \\\\\n",
    "-0.0486 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix} 0.4796 \\\\ \n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "If we calculate the above matrix multiplication and then broadcast the bias vector, we get:\n",
    "\n",
    "$$Z^{[2]} = \\begin{bmatrix} 0.54461443 \\\\\n",
    "0.73008593 \\\\\n",
    "0.47989564 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Each row of the matrix $Z^{[2]}$ corresponds to a single example in the batch. This is the linear transformation of the first layer's output by the second layer of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2: [[0.54461443]\n",
      " [0.73008593]\n",
      " [0.47989564]]\n"
     ]
    }
   ],
   "source": [
    "Z2 = np.matmul(A1, W2.T) + b2\n",
    "print(f\"Z2: {Z2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function for Layer 2\n",
    "\n",
    "$$A^{[2]} = g(Z^{[2]})$$\n",
    "\n",
    "$$A^{[2]} = \\begin{bmatrix} \\sigma(0.54461443) \\\\\n",
    "\\sigma(0.73008593) \\\\\n",
    "\\sigma(0.47989564) \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If we calculate the sigmoid function for each value, we get:\n",
    "\n",
    "\n",
    "$$A^{[2]} = \\begin{bmatrix} 0.6328852 \\\\\n",
    "0.67482413 \\\\\n",
    "0.61772323 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix $A^{[2]}$ is the output of the second layer (output layer).\n",
    "- Each row of the matrix $A^{[2]}$ corresponds to a single example in the batch. \n",
    "- Each column is a activation value of a neuron in the layer for all examples. We have 1 neuron in the second layer, so we have 1 column in the output matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2: [[0.6328852 ]\n",
      " [0.67482413]\n",
      " [0.61772323]]\n"
     ]
    }
   ],
   "source": [
    "A2 = sigmoid(Z2)\n",
    "print(f\"A2: {A2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results with the PyTorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:\n",
      "tensor([[0.6329],\n",
      "        [0.6748],\n",
      "        [0.6177]])\n",
      "Manual:\n",
      "[[0.6328852 ]\n",
      " [0.67482413]\n",
      " [0.61772323]]\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of PyTorch and Manual Calculation\n",
    "\n",
    "print(f\"PyTorch:\\n{y_pred_torch.data}\")\n",
    "print(f\"Manual:\\n{A2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of the Neural Network\n",
    "In this example, our neural network have 2 layers. So, the output of the second layer is in fact the output of the neural network.\n",
    "\n",
    "Output of the neural network:\n",
    "\n",
    "$$A^{[2]} = \\begin{bmatrix} 0.6328852 \\\\\n",
    "0.67482413 \\\\\n",
    "0.61772323 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Since we have 3 examples in the batch, each of the rows in the output matrix corresponds to the output of the neural network for a single example.\n",
    "\n",
    "- ${a^{[2]}_{1}}^{(1)} = 0.6328852$ is the output of the neural network for the first example.\n",
    "- ${a^{[2]}_{1}}^{(2)} = 0.67482413$ is the output of the neural network for the second example.\n",
    "- ${a^{[2]}_{1}}^{(3)} = 0.61772323$ is the output of the neural network for the third example.\n",
    "\n",
    "#### Derive $\\hat{y}$ from the output of the neural network\n",
    "Deriving the final output $\\hat{y}$ depends on the task and type of the activation function in the output layer. For example, in a binary classification task, we can use the Sigmoid activation function in the output layer (like in this example). Then the output of the neural network is the probability of the positive class. \n",
    "\n",
    "$$P(y = 1 | X) = A^{[2]}$$\n",
    "\n",
    "If we set the threshold to 0.5:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } A^{[2]} \\geq 0.5 \\\\ 0 & \\text{if } A^{[2]} < 0.5 \\end{cases}$$\n",
    "\n",
    "So:\n",
    "- For the first example, $0.6328852 \\geq 0.5$, so $\\hat{y}^{(1)} = 1$.\n",
    "- For the second example, $0.67482413 \\geq 0.5$, so $\\hat{y}^{(2)} = 1$.\n",
    "- For the third example, $0.61772323 \\geq 0.5$, so $\\hat{y}^{(3)} = 1$.\n",
    "\n",
    "In simple words, the prediction of the neural network for the first example is 1, for the second example is 1, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Decisions:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Manual Calculation Decisions:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_yhat(y_pred):\n",
    "    yhat = np.zeros_like(y_pred)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] >= 0.5:\n",
    "            yhat[i] = 1\n",
    "        else:\n",
    "            yhat[i] = 0\n",
    "    return yhat\n",
    "\n",
    "\n",
    "# Network decisions for PyTorch\n",
    "print(f\"PyTorch Decisions:\\n{calculate_yhat(y_pred_torch.data)}\")\n",
    "\n",
    "# Network decisions for Manual Calculation\n",
    "print(f\"Manual Calculation Decisions:\\n{calculate_yhat(A2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "In the above, we used the following notation for the linear transformation:\n",
    "\n",
    "$$Z^{[l]} = A^{[l-1]}{W^{[l]}}^\\top + \\vec{\\mathbf{b}}^{[l]}$$\n",
    "\n",
    "Where each row of input matrix $A^{[l-1]}$ and output matrix $Z^{[l]}$ corresponds to a single example in the batch. This notation aligns with the actual implementation of deep learning frameworks like PyTorch and TensorFlow.\n",
    "\n",
    "However, in some text you may see the following notation:\n",
    "\n",
    "$$Z^{[l]} = {{W^{[l]}}^\\top}A^{[l-1]} + \\vec{\\mathbf{b}}^{[l]}$$\n",
    "\n",
    "This is mathematically equivalent to the first notation with the only difference that examples are represented as columns instead of rows, and the weight matrix $W^{[l]}$ is transposed accordingly.\n",
    "\n",
    "The output matrix of the linear transformation $Z^{[l]}$ will have each **column** corresponding to a single example in the batch.\n",
    "\n",
    "See this in action below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 1.25  0.38]\n",
      " [-0.45  3.01]\n",
      " [ 0.72 -0.56]]\n",
      "W1:\n",
      "[[-0.6053  0.2325]\n",
      " [-0.5255 -0.6182]\n",
      " [ 0.0117 -0.1774]]\n",
      "b1:\n",
      "[ 0.3849 -0.6344 -0.2022]\n",
      "Z1:\n",
      "[[-0.283375 -1.526191 -0.254987]\n",
      " [ 1.35711  -2.258707 -0.741439]\n",
      " [-0.181116 -0.666568 -0.094432]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"X:\\n{X}\")\n",
    "print(f\"W1:\\n{W1}\")\n",
    "print(f\"b1:\\n{b1}\")\n",
    "print(f\"Z1:\\n{Z1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following:\n",
    "- $X$ is the input matrix with 3 examples and 2 features each. Each **column** corresponds to a single example.\n",
    "- $W^{[1]}$ is the weight matrix of the first layer with 3 neurons and 2 input features. Since the `W1` is already has the shape of $(3, 2)$, we don't need to transpose it.\n",
    "- $b^{[1]}$ is the bias column vector of the first layer with 3 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise Z1:\n",
      "[[-0.283375  1.35711  -0.181116]\n",
      " [-1.526191 -2.258707 -0.666568]\n",
      " [-0.254987 -0.741439 -0.094432]]\n"
     ]
    }
   ],
   "source": [
    "# Examples are in columns (3 examples with 2 features)\n",
    "col_X = np.array([[1.25, -0.45, 0.72], [0.38, 3.01, -0.56]])\n",
    "\n",
    "# Weights in columns. Each column represents the weights of a neuron\n",
    "col_W1 = np.array([[-0.6053, -0.5255, 0.0117], [0.2325, -0.6182, -0.1774]])\n",
    "\n",
    "# Biases are in a column vector.\n",
    "col_b1 = np.array([[0.3849], [-0.6344], [-0.2022]])\n",
    "\n",
    "col_Z1 = np.matmul(col_W1.T, col_X) + col_b1\n",
    "\n",
    "print(f\"Column-wise Z1:\\n{col_Z1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result of the linear transformation with the previous calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(Z1, col_Z1.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
