{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Backpropagation and Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Graph with One Parameter\n",
    "Let's start with simple computation graph. Consider composite function $f(x)$ as follows:\n",
    "\n",
    "$$f(x) = ((x + 5)^3+1)^2$$\n",
    "\n",
    "And let's compute our function $f(x)$ at the point $x = -3$. So, if we decompose our function $f(x)$ into simple functions, we get:\n",
    "\n",
    "$$h= x + 5 = 2$$\n",
    "$$g = h^3 + 1 = 9$$\n",
    "$$f = g^2 = 81$$\n",
    "\n",
    "See [Computational Graph]() for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use PyTorch to compute the output (forward propagation) of our function $f(x)$ at the point $x = -3$. Then we will compute the gradients (backward propagation) of our function $f(x)$ with respect to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to compute the gradient (derivative) of our function $f(x)$ with respect to $x$, we need to set `requires_grad=True` for any parameter that will be part of the computation graph for which we want to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "f_x = ((x + 5) ** 3 + 1) ** 2\n",
    "print(f_x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: 216.0\n"
     ]
    }
   ],
   "source": [
    "f_x.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"df/dx: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward propagation, PyTorch automatically builds a directed acyclic graph (DAG) of operations. This is when we calculate `f_x`. PyTorch create this graph when `torch.tensor` with `requires_grad=True` is involved in the computation.\n",
    "\n",
    "When we call `f_x.backward()`, it walks backward through the computational graph to to each parameter (leaf node) and apply the chain rule of calculus to compute the gradients of the output (the final node where we called `backward()`) with respect to each parameter.\n",
    "\n",
    "Gradient Accumulation: The calculated gradients are stored in the `grad` attribute of each parameter tensor that has `requires_grad=True`.\n",
    "\n",
    "This [PyTorch Computation Graph](https://www.youtube.com/watch?v=MswxJw-8PvE) is a good video on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a simple linear regression model with one input feature $x$ and one target $y$. The model is defined as follows:\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where $w$ is the weight and $b$ is the bias. \n",
    "\n",
    "The [cost function]() is defined as the mean squared error (MSE) for a dataset with only one sample:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2}(f_{w,b}(x) - y)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Descent]() is the most common optimization algorithm used to minimize the cost function. The algorithm works by iteratively updating the parameters in the opposite direction of the gradients of the cost function with respect to the parameters. So, we need to compute the gradients of the cost function with respect to the parameters $w$ and $b$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Computational Graph]() for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our input $x$, target value $y$, weight $w$ and bias $b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0)\n",
    "y = torch.tensor(5.0)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't set the `requires_grad=True` for $x$ and $y$ because they are the input and target values. They are not parameters of the model which we need to optimize by computing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Forward Propagation (Compute Model Output)\n",
    "We'll compute the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation (compute the output)\n",
    "c = w * x\n",
    "f = c + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Forward Propagation (Compute the Cost Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cost\n",
    "d = f - y\n",
    "J = (d**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the intermediate gradients for non-leaf nodes are not stored by default which is more efficient in practice when we have a large number of parameters. However, in this example to demonstrate the backward steps in the computational graph, we will change this behavior by calling `retain_grad()` on the intermediate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retain_grad()\n",
    "f.retain_grad()\n",
    "d.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature x and target y:\n",
      "x: -3.0\n",
      "y: 5.0\n",
      "\n",
      "Model parameters:\n",
      "w: 3.0\n",
      "b: 1.0\n",
      "\n",
      "Model output:\n",
      "c: -9.0\n",
      "f: -8.0\n",
      "\n",
      "Cost:\n",
      "d: -13.0\n",
      "J: 84.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input feature x and target y:\\nx: {x.data}\\ny: {y.data}\\n\")\n",
    "print(f\"Model parameters:\\nw: {w.data}\\nb: {b.data}\\n\")\n",
    "print(f\"Model output:\\nc: {c.data}\\nf: {f.data}\\n\")\n",
    "print(f\"Cost:\\nd: {d.data}\\nJ: {J.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Backpropagation (Compute the Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (compute the gradients)\n",
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ/dd: -13.0\n",
      "dJ/df: -13.0\n",
      "dJ/dc: -13.0\n",
      "dJ/db: -13.0\n",
      "dJ/dw: 39.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"dJ/dd: {d.grad}\")\n",
    "print(f\"dJ/df: {f.grad}\")\n",
    "print(f\"dJ/dc: {c.grad}\")\n",
    "print(f\"dJ/db: {b.grad}\")\n",
    "print(f\"dJ/dw: {w.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "Let's now see the computational graph for a simple neural network with the following architecture:\n",
    "- Layer 0 (input layer): 2 features\n",
    "- Layer 1: Fully connected layer with 3 neurons and ReLU activation function.\n",
    "- Layer 2: Fully connected layer with 2 neurons and ReLU activation function.\n",
    "- Layer 3 (output): Fully connected layer with 1 neuron (output) and Sigmoid activation function.\n",
    "\n",
    "In this example we use a batch dataset with 2 samples. The input $X$ and target $Y$ are defined as follows:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "Which means, for example 1 $x_1 = 1$ and $x_2 = 2$ and the target class $y = 0$.\n",
    "\n",
    "Recall that we maintain each sample in **rows** and features in **columns**. So, each row of $X$ and $Y$ is associated with one sampleDataset with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = torch.tensor([[0.0], [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Define the model architecture (Layers and nodes)\n",
    "        self.linear1 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.linear2 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.linear3 = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward Propagation happens here.\n",
    "        # It takes the input tensor x and returns the output tensor for each\n",
    "        # layer by applying the linear transformation first and then the\n",
    "        # activation function.\n",
    "        # It start from layer 1 and goes forward layer by layer to the output\n",
    "        # layer.\n",
    "\n",
    "        # Layer 1 linear transformation\n",
    "        Z1 = self.linear1(x)\n",
    "        # Layer 1 activation\n",
    "        A1 = F.relu(Z1)\n",
    "\n",
    "        # Layer 2 linear transformation\n",
    "        Z2 = self.linear2(A1)\n",
    "        # Layer 2 activation\n",
    "        A2 = F.relu(Z2)\n",
    "\n",
    "        # Layer 3 (output layer) linear transformation\n",
    "        Z3 = self.linear3(A2)\n",
    "        # Layer 3 activation\n",
    "        A3 = F.sigmoid(Z3)\n",
    "\n",
    "        # Print the intermediate results\n",
    "        print(\n",
    "            f\"Z1:\\n{Z1}\\nA1:\\n{A1}\\nZ2:\\n{Z2}\\nA2:\\n{A2}\\nZ3:\\n{Z3}\\nA3:\\n{A3}\"\n",
    "        )\n",
    "\n",
    "        # Output of the model (prediction)\n",
    "        return A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for binary classification problems, when we use the Sigmoid activation function in the output layer, we defer the computation of output layer activation to outside the model. In that way, the model only outputs the **logits** of the output layer. Then the logits are passed to the Sigmoid activation function to get the predicted probabilities, or for training to the  `BCEWithLogitsLoss` binary cross-entropy loss function which combines the Sigmoid activation function and the binary cross-entropy loss.\n",
    "\n",
    "That approach is more numerically stable and typically gives better training results. However, in this example, for easier demonstration, we will include the Sigmoid activation function in the model's output layer and use the `BCELoss` loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (linear3): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the initial weights and biases of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-0.0669,  0.6420],\n",
      "        [-0.5951,  0.0679],\n",
      "        [-0.7008,  0.4456]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([-0.3008, -0.3751,  0.1771], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5553,  0.5280, -0.5235],\n",
      "        [-0.4386, -0.1877, -0.3298]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-0.4900,  0.4226], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.1257, -0.4513]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([-0.5929], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    for i, child in enumerate(model.children()):\n",
    "        print(f\"Layer {i+1}: {type(child).__name__}\")\n",
    "        child_parameters = dict(child.named_parameters())\n",
    "\n",
    "        for name, param in child_parameters.items():\n",
    "            print(f\"\\n{name}: {param.size()} {param}\")\n",
    "            print(f\"{name}.grad:\\n{param.grad}\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, gradients of parameters are `None` since we haven't computed any gradients yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example for simplicity and having reproducible results, we'll set the weights and biases manually. Let's say we have the following weights and biases:\n",
    "\n",
    "Similar to the way that PyTorch creates weights matrices:\n",
    "- Each row of $W^{[l]}$ is associated with one neuron in the layer $l$. For example, in layer 1, We have 3 neurons, so we have 3 rows in $W^{[1]}$.\n",
    "- Each column is associated with one feature of input values. For example, the number of columns in $W^{[1]}$ is equal to the number of features in the input layer. We have 2 features in the input layer $X$, so we have 2 columns in $W^{[1]}$, and so on.\n",
    "\n",
    "Note: The number of weight and biases are independent of the number of samples in the batch or entire dataset. The whole point is to optimize these parameters by exposing them to the entire dataset through cycle of forward and backward propagation. So, no matter what is the size of the dataset, the number of parameters in the model is fixed and defined by the architecture of the neural network.\n",
    "\n",
    "**Layer 1 (3 neurons):**\n",
    "$$W^{[1]} = \\begin{bmatrix} -1 & 2 \\\\ 3 & 0.5 \\\\ -0.1 & -4\\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[1]} = \\begin{bmatrix} 1 & -2 & 0.3 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**Layer 2 (2 neurons):**\n",
    "$$W^{[2]} = \\begin{bmatrix} 0.5 & 1 & -2 \\\\ 0.7 & 0.1 & 0.3\\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[2]} = \\begin{bmatrix} -4 & 5 \\end{bmatrix}$$\n",
    "\n",
    "**Layer 3 (output):**\n",
    "$$W^{[3]} = \\begin{bmatrix} 0.5 & -0.3 \\end{bmatrix} \\quad {\\vec{\\mathbf{b}}}^{[3]} = \\begin{bmatrix} 0.1 \\end{bmatrix}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "W_1 = torch.tensor([[-1.0, 2.0], [3.0, 0.5], [-0.1, -4.0]], requires_grad=True)\n",
    "b_1 = torch.tensor([1.0, -2.0, 0.3], requires_grad=True)\n",
    "\n",
    "# Layer 2\n",
    "W_2 = torch.tensor([[0.5, 1.0, -2.0], [0.7, 0.1, 0.3]], requires_grad=True)\n",
    "b_2 = torch.tensor([-4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Layer 3 (Output layer)\n",
    "W_3 = torch.tensor([[0.5, -0.3]], requires_grad=True)\n",
    "b_3 = torch.tensor([0.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.linear1.weight.data.copy_(W_1)\n",
    "model.linear1.bias.data.copy_(b_1)\n",
    "\n",
    "model.linear2.weight.data.copy_(W_2)\n",
    "model.linear2.bias.data.copy_(b_2)\n",
    "\n",
    "model.linear3.weight.data.copy_(W_3)\n",
    "model.linear3.bias.data.copy_(b_3)\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the forward propagation using the current weights and biases, and the input $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1:\n",
      "tensor([[  4.0000,   2.0000,  -7.8000],\n",
      "        [  6.0000,   9.0000, -16.0000]], grad_fn=<AddmmBackward0>)\n",
      "A1:\n",
      "tensor([[4., 2., 0.],\n",
      "        [6., 9., 0.]], grad_fn=<ReluBackward0>)\n",
      "Z2:\n",
      "tensor([[ 0.0000,  8.0000],\n",
      "        [ 8.0000, 10.1000]], grad_fn=<AddmmBackward0>)\n",
      "A2:\n",
      "tensor([[ 0.0000,  8.0000],\n",
      "        [ 8.0000, 10.1000]], grad_fn=<ReluBackward0>)\n",
      "Z3:\n",
      "tensor([[-2.3000],\n",
      "        [ 1.0700]], grad_fn=<AddmmBackward0>)\n",
      "A3:\n",
      "tensor([[0.0911],\n",
      "        [0.7446]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.19522885978221893\n"
     ]
    }
   ],
   "source": [
    "cost = F.binary_cross_entropy(output, Y)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zero_grad()` simply resets (zeros out) the gradients of all parameters. Here, we don't need it since we haven't computed any gradients yet, and the gradients are `None`, however this is a good practice to reset the gradients before any new gradients computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "None\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (compute the gradients)\n",
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "\n",
      "weight: torch.Size([3, 2]) Parameter containing:\n",
      "tensor([[-1.0000,  2.0000],\n",
      "        [ 3.0000,  0.5000],\n",
      "        [-0.1000, -4.0000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-0.0249, -0.0396],\n",
      "        [-0.1814, -0.2428],\n",
      "        [ 0.0000,  0.0000]])\n",
      "\n",
      "bias: torch.Size([3]) Parameter containing:\n",
      "tensor([ 1.0000, -2.0000,  0.3000], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0147, -0.0614,  0.0000])\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "\n",
      "weight: torch.Size([2, 3]) Parameter containing:\n",
      "tensor([[ 0.5000,  1.0000, -2.0000],\n",
      "        [ 0.7000,  0.1000,  0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-0.3831, -0.5747,  0.0000],\n",
      "        [ 0.1752,  0.3175,  0.0000]])\n",
      "\n",
      "bias: torch.Size([2]) Parameter containing:\n",
      "tensor([-4.,  5.], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0639,  0.0246])\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "\n",
      "weight: torch.Size([1, 2]) Parameter containing:\n",
      "tensor([[ 0.5000, -0.3000]], requires_grad=True)\n",
      "weight.grad:\n",
      "tensor([[-1.0216, -0.9253]])\n",
      "\n",
      "bias: torch.Size([1]) Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n",
      "bias.grad:\n",
      "tensor([-0.0821])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
