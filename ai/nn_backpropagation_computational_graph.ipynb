{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Backpropagation and Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Graph with One Parameter\n",
    "Let's start with simple computation graph. Consider composite function $f(x)$ as follows:\n",
    "\n",
    "$$f(x) = ((x + 5)^3+1)^2$$\n",
    "\n",
    "And let's compute our function $f(x)$ at the point $x = -3$. So, if we decompose our function $f(x)$ into simple functions, we get:\n",
    "\n",
    "$$h= x + 5 = 2$$\n",
    "$$g = h^3 + 1 = 9$$\n",
    "$$f = g^2 = 81$$\n",
    "\n",
    "See [Computational Graph]() for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use PyTorch to compute the output (forward propagation) of our function $f(x)$ at the point $x = -3$. Then we will compute the gradients (backward propagation) of our function $f(x)$ with respect to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to compute the gradient (derivative) of our function $f(x)$ with respect to $x$, we need to set `requires_grad=True` for any parameter that will be part of the computation graph for which we want to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "f_x = ((x + 5) ** 3 + 1) ** 2\n",
    "print(f_x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: 216.0\n"
     ]
    }
   ],
   "source": [
    "f_x.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"df/dx: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward propagation, PyTorch automatically builds a directed acyclic graph (DAG) of operations. This is when we calculate `f_x`. PyTorch create this graph when `torch.tensor` with `requires_grad=True` is involved in the computation.\n",
    "\n",
    "When we call `f_x.backward()`, it walks backward through the computational graph to to each parameter (leaf node) and apply the chain rule of calculus to compute the gradients of the output (the final node where we called `backward()`) with respect to each parameter.\n",
    "\n",
    "Gradient Accumulation: The calculated gradients are stored in the `grad` attribute of each parameter tensor that has `requires_grad=True`.\n",
    "\n",
    "This [PyTorch Computation Graph](https://www.youtube.com/watch?v=MswxJw-8PvE) is a good video on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a simple linear regression model with one input feature $x$ and one target $y$. The model is defined as follows:\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where $w$ is the weight and $b$ is the bias. \n",
    "\n",
    "The [cost function]() is defined as the mean squared error (MSE) for a dataset with only one sample:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2}(f_{w,b}(x) - y)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Descent]() is the most common optimization algorithm used to minimize the cost function. The algorithm works by iteratively updating the parameters in the opposite direction of the gradients of the cost function with respect to the parameters. So, we need to compute the gradients of the cost function with respect to the parameters $w$ and $b$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Computational Graph]() for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our input $x$, target value $y$, weight $w$ and bias $b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0)\n",
    "y = torch.tensor(5.0)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't set the `requires_grad=True` for $x$ and $y$ because they are the input and target values. They are not parameters of the model which we need to optimize by computing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Forward Propagation (Compute Model Output)\n",
    "We'll compute the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation (compute the output)\n",
    "c = w * x\n",
    "f = c + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Forward Propagation (Compute the Cost Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cost\n",
    "d = f - y\n",
    "J = (d**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the intermediate gradients for non-leaf nodes are not stored by default which is more efficient in practice when we have a large number of parameters. However, in this example to demonstrate the backward steps in the computational graph, we will change this behavior by calling `retain_grad()` on the intermediate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retain_grad()\n",
    "f.retain_grad()\n",
    "d.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature x and target y:\n",
      "x: -3.0\n",
      "y: 5.0\n",
      "\n",
      "Model parameters:\n",
      "w: 3.0\n",
      "b: 1.0\n",
      "\n",
      "Model output:\n",
      "c: -9.0\n",
      "f: -8.0\n",
      "\n",
      "Cost:\n",
      "d: -13.0\n",
      "J: 84.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input feature x and target y:\\nx: {x.data}\\ny: {y.data}\\n\")\n",
    "print(f\"Model parameters:\\nw: {w.data}\\nb: {b.data}\\n\")\n",
    "print(f\"Model output:\\nc: {c.data}\\nf: {f.data}\\n\")\n",
    "print(f\"Cost:\\nd: {d.data}\\nJ: {J.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Backpropagation (Compute the Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (compute the gradients)\n",
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ/dd: -13.0\n",
      "dJ/df: -13.0\n",
      "dJ/dc: -13.0\n",
      "dJ/db: -13.0\n",
      "dJ/dw: 39.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"dJ/dd: {d.grad}\")\n",
    "print(f\"dJ/df: {f.grad}\")\n",
    "print(f\"dJ/dc: {c.grad}\")\n",
    "print(f\"dJ/db: {b.grad}\")\n",
    "print(f\"dJ/dw: {w.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "Let's now see the computational graph for a simple neural network with the following architecture:\n",
    "- Layer 0 (input layer): 2 features\n",
    "- Layer 1: Fully connected layer with 3 neurons and ReLU activation function.\n",
    "- Layer 2: Fully connected layer with 2 neurons and ReLU activation function.\n",
    "- Layer 3 (output): Fully connected layer with 1 neuron (output) and Sigmoid activation function.\n",
    "\n",
    "In this example we use a batch dataset with 2 samples. The input $X$ and target $Y$ are defined as follows:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$$\n",
    "$$Y = \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix}$$\n",
    "\n",
    "Which means, for example 1 $x_1 = 1$ and $x_2 = 2$ and the target $y = 0.5$.\n",
    "\n",
    "Recall that we maintain each sample in **rows** and features in **columns**. So, each row of $X$ and $Y$ is associated with one sampleDataset with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = torch.tensor([[0.5], [0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Define the model architecture (Layers and nodes)\n",
    "        self.linear1 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.linear2 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.linear3 = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward Propagation happens here.\n",
    "        # It takes the input tensor x and returns the output tensor for each\n",
    "        # layer by applying the linear transformation first and then the\n",
    "        # activation function.\n",
    "        # It start from layer 1 and goes forward layer by layer to the output\n",
    "        # layer.\n",
    "\n",
    "        # Layer 1 linear transformation\n",
    "        Z1 = self.linear1(x)\n",
    "        # Layer 1 activation\n",
    "        A1 = F.relu(Z1)\n",
    "\n",
    "        # Layer 2 linear transformation\n",
    "        Z2 = self.linear2(A1)\n",
    "        # Layer 2 activation\n",
    "        A2 = F.relu(Z2)\n",
    "\n",
    "        # Layer 3 (output layer) linear transformation\n",
    "        Z3 = self.linear3(A2)\n",
    "        # Layer 3 activation\n",
    "        A3 = F.sigmoid(Z3)\n",
    "\n",
    "        # Print the intermediate results\n",
    "        print(f\"Z1: {Z1}\\nA1: {A1}\\nZ2: {Z2}\\nA2: {A2}\\nZ3: {Z3}\\nA3: {A3}\")\n",
    "\n",
    "        # Output of the model (prediction)\n",
    "        return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (linear3): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the initial weights and biases of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4328, -0.2531],\n",
      "        [ 0.5298, -0.4185],\n",
      "        [-0.4404,  0.3311]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.linear1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Linear\n",
      "weights: Parameter containing:\n",
      "tensor([[ 0.4328, -0.2531],\n",
      "        [ 0.5298, -0.4185],\n",
      "        [-0.4404,  0.3311]], requires_grad=True)\n",
      "bias: Parameter containing:\n",
      "tensor([0.3261, 0.6766, 0.3776], requires_grad=True)\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 2: Linear\n",
      "weights: Parameter containing:\n",
      "tensor([[ 0.5389,  0.5690, -0.1213],\n",
      "        [-0.2766,  0.0664,  0.1120]], requires_grad=True)\n",
      "bias: Parameter containing:\n",
      "tensor([-0.0097, -0.3818], requires_grad=True)\n",
      "--------------------------------------------------------------------------------\n",
      "Layer 3: Linear\n",
      "weights: Parameter containing:\n",
      "tensor([[-0.3824,  0.1682]], requires_grad=True)\n",
      "bias: Parameter containing:\n",
      "tensor([0.2470], requires_grad=True)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    for i, child in enumerate(model.children()):\n",
    "        print(f\"Layer {i+1}: {type(child).__name__}\")\n",
    "        child_parameters = dict(child.named_parameters())\n",
    "\n",
    "        print(f\"weights: {child_parameters['weight']}\")\n",
    "        print(f\"bias: {child_parameters['bias']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example for simplicity and having reproducible results, we'll set the weights and biases manually. Let's say we have the following weights and biases:\n",
    "\n",
    "**Layer 1:**\n",
    "$$W_1 = \\begin{bmatrix} -1 & 2 \\\\ 4 & 5 \\\\ 6 & -3\\end{bmatrix} \\quad b_1 = \\begin{bmatrix} -1 & -2 & 3 \\end{bmatrix}$$\n",
    "\n",
    "Note: each row of $W_1$ is associated with one neuron in layer 1. We have 3 neurons in layer 1, so we have 3 rows in $W_1$. The number of columns in $W_1$ is equal to the number of features in the input layer. We have 2 features in the input layer $X$, so we have 2 columns in $W_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
