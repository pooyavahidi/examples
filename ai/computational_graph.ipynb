{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Graph with One Parameter\n",
    "Let's start with simple computation graph. Consider composite function $f(x)$ as follows:\n",
    "\n",
    "$$f(x) = ((x + 5)^3+1)^2$$\n",
    "\n",
    "And let's compute our function $f(x)$ at the point $x = -3$. So, if we decompose our function $f(x)$ into simple functions, we get:\n",
    "\n",
    "$$h= x + 5 = 2$$\n",
    "$$g = h^3 + 1 = 9$$\n",
    "$$f = g^2 = 81$$\n",
    "\n",
    "See [Computational Graph]() for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use PyTorch to compute the output (forward propagation) of our function $f(x)$ at the point $x = -3$. Then we will compute the gradients (backward propagation) of our function $f(x)$ with respect to $x$.\n",
    "\n",
    "Note: We use PyTorch in this example to merely utilize its differentitation features and compute the gradients. This is completely optional and you can compute the gradients manually or using other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to compute the gradient (derivative) of our function $f(x)$ with respect to $x$, we need to set `requires_grad=True` for any parameter that will be part of the computation graph for which we want to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "f_x = ((x + 5) ** 3 + 1) ** 2\n",
    "print(f_x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: 216.0\n"
     ]
    }
   ],
   "source": [
    "f_x.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"df/dx: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward propagation, PyTorch automatically builds a directed acyclic graph (DAG) of operations. This is when we calculate `f_x`. PyTorch create this graph when `torch.tensor` with `requires_grad=True` is involved in the computation.\n",
    "\n",
    "When we call `f_x.backward()`, it walks backward through the computational graph to to each parameter (leaf node) and apply the chain rule of calculus to compute the gradients of the output (the final node where we called `backward()`) with respect to each parameter.\n",
    "\n",
    "Gradient Accumulation: The calculated gradients are stored in the `grad` attribute of each parameter tensor that has `requires_grad=True`.\n",
    "\n",
    "This [PyTorch Computation Graph](https://www.youtube.com/watch?v=MswxJw-8PvE) is a good video on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a simple linear regression model with one input feature $x$ and one target $y$. The model is defined as follows:\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where $w$ is the weight and $b$ is the bias. \n",
    "\n",
    "The [cost function]() is defined as the mean squared error (MSE) for a dataset with only one sample:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2}(f_{w,b}(x) - y)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Descent]() is the most common optimization algorithm used to minimize the cost function. The algorithm works by iteratively updating the parameters in the opposite direction of the gradients of the cost function with respect to the parameters. So, we need to compute the gradients of the cost function with respect to the parameters $w$ and $b$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Computational Graph]() for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our input $x$, target value $y$, weight $w$ and bias $b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3.0)\n",
    "y = torch.tensor(5.0)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't set the `requires_grad=True` for $x$ and $y$ because they are the input and target values. They are not parameters of the model which we need to optimize by computing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Forward Propagation (Compute Model Output)\n",
    "We'll compute the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation (compute the output)\n",
    "c = w * x\n",
    "f = c + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Compute the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cost\n",
    "d = f - y\n",
    "J = (d**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the intermediate gradients for non-leaf nodes are not stored by default which is more efficient in practice when we have a large number of parameters. However, in this example to demonstrate the backward steps in the computational graph, we will change this behavior by calling `retain_grad()` on the intermediate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retain_grad()\n",
    "f.retain_grad()\n",
    "d.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature x and target y:\n",
      "x: -3.0\n",
      "y: 5.0\n",
      "\n",
      "Model parameters:\n",
      "w: 3.0\n",
      "b: 1.0\n",
      "\n",
      "Model output:\n",
      "c: -9.0\n",
      "f: -8.0\n",
      "\n",
      "Cost:\n",
      "d: -13.0\n",
      "J: 84.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input feature x and target y:\\nx: {x.data}\\ny: {y.data}\\n\")\n",
    "print(f\"Model parameters:\\nw: {w.data}\\nb: {b.data}\\n\")\n",
    "print(f\"Model output:\\nc: {c.data}\\nf: {f.data}\\n\")\n",
    "print(f\"Cost:\\nd: {d.data}\\nJ: {J.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Backpropagation (Compute the Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (compute the gradients)\n",
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ/dd: -13.0\n",
      "dJ/df: -13.0\n",
      "dJ/dc: -13.0\n",
      "dJ/db: -13.0\n",
      "dJ/dw: 39.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"dJ/dd: {d.grad}\")\n",
    "print(f\"dJ/df: {f.grad}\")\n",
    "print(f\"dJ/dc: {c.grad}\")\n",
    "print(f\"dJ/db: {b.grad}\")\n",
    "print(f\"dJ/dw: {w.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
